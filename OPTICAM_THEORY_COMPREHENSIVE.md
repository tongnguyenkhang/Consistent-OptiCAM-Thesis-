# TÃ i Liá»‡u LÃ½ Thuyáº¿t Äáº§y Äá»§: OptiCAM vÃ  Multi-Component OptiCAM

## Má»¥c Lá»¥c
0. [Báº£ng KÃ½ Hiá»‡u vÃ  Thuáº­t Ngá»¯](#0-báº£ng-kÃ½-hiá»‡u-vÃ -thuáº­t-ngá»¯)
1. [Tá»•ng Quan vÃ  Äá»™ng CÆ¡](#1-tá»•ng-quan-vÃ -Ä‘á»™ng-cÆ¡)
2. [OptiCAM Baseline - LÃ½ Thuyáº¿t Ná»n Táº£ng](#2-opticam-baseline---lÃ½-thuyáº¿t-ná»n-táº£ng)
3. [Multi-Component OptiCAM - Má»Ÿ Rá»™ng](#3-multi-component-opticam---má»Ÿ-rá»™ng)
4. [HÃ m Má»¥c TiÃªu vÃ  Loss Functions](#4-hÃ m-má»¥c-tiÃªu-vÃ -loss-functions)
5. [Tá»‘i Æ¯u HÃ³a: Adam Optimizer vÃ  Mixed Precision](#5-tá»‘i-Æ°u-hÃ³a-adam-optimizer-vÃ -mixed-precision)
6. [Metrics ÄÃ¡nh GiÃ¡](#6-metrics-Ä‘Ã¡nh-giÃ¡)
7. [Váº¥n Äá» Quan Trá»ng: num_masks - K Components vs C Channels](#7-váº¥n-Ä‘á»-quan-trá»ng-num_masks---k-components-vs-c-channels)

---

## 0. Báº£ng KÃ½ Hiá»‡u vÃ  Thuáº­t Ngá»¯

### 0.1. KÃ½ Hiá»‡u ToÃ¡n Há»c

#### Input vÃ  Output
| KÃ½ hiá»‡u | Dimension | Ã nghÄ©a | VÃ­ dá»¥ |
|---------|-----------|---------|-------|
| $\mathbf{x}$ | $\mathbb{R}^{3 \times H \times W}$ | Input image (RGB) | $224 \times 224$ pixels |
| $\mathcal{X}$ | - | Image space (táº­p há»£p táº¥t cáº£ áº£nh) | - |
| $H, W$ | scalar | Height, Width cá»§a áº£nh | $H = W = 224$ |
| $c$ | scalar | Target class index | $c \in \{0, 1, ..., C-1\}$ |
| $C$ | scalar | Total number of classes | ImageNet: $C = 1000$ |

#### Network Architecture
| KÃ½ hiá»‡u | Dimension | Ã nghÄ©a | VÃ­ dá»¥ |
|---------|-----------|---------|-------|
| $f(\cdot)$ | $\mathcal{X} \to \mathbb{R}^C$ | CNN classifier (toÃ n bá»™ network) | ResNet50 |
| $\mathbf{y}$ hoáº·c $\text{logits}$ | $\mathbb{R}^C$ | Logit vector (raw scores) | $[-5.2, 8.1, ...]$ |
| $y_c$ | scalar | Logit cá»§a class $c$ | $y_{\text{dog}} = 8.1$ |
| $\ell$ | - | Layer index | layer4[-1] |
| $f_\ell(\cdot)$ | $\mathcal{X} \to \mathbb{R}^{K_\ell \times h_\ell \times w_\ell}$ | Feature extractor Ä‘áº¿n layer $\ell$ | ResNet50 Ä‘áº¿n layer4 |

#### Feature Maps
| KÃ½ hiá»‡u | Dimension | Ã nghÄ©a | Giáº£i thÃ­ch |
|---------|-----------|---------|-----------|
| $A^k_\ell$ | $\mathbb{R}^{h_\ell \times w_\ell}$ | Feature map cho channel $k$ táº¡i layer $\ell$ | 1 trong 2048 channels |
| $K_\ell$ | scalar | Sá»‘ channels táº¡i layer $\ell$ | ResNet50 layer4: $K_\ell = 2048$ |
| $h_\ell, w_\ell$ | scalar | Spatial dimensions cá»§a feature maps | $14 \times 14$ (tá»« input $224 \times 224$) |
| $\mathbf{f}$ hoáº·c $\mathbf{A}_\ell$ | $\mathbb{R}^{K_\ell \times h_\ell \times w_\ell}$ | ToÃ n bá»™ feature maps táº¡i layer $\ell$ | Tensor 3D |

#### Saliency Maps vÃ  Masks

**âš ï¸ LÆ¯U Ã:** Multi-Component dÃ¹ng index **j** cho components ($j=1..K$), khÃ¡c vá»›i channel index **k** ($k=1..K_\ell$).

| KÃ½ hiá»‡u | Dimension | Ã nghÄ©a | Giáº£i thÃ­ch |
|---------|-----------|---------|-----------|
| $S_\ell$ | $\mathbb{R}^{h_\ell \times w_\ell}$ | Saliency map (feature resolution) - Baseline | $14 \times 14$ |
| $S_\ell^{(j)}$ | $\mathbb{R}^{h_\ell \times w_\ell}$ | Saliency map cho **component** $j$ | Multi-component only, $j=1..K$ |
| $m$ | $[0,1]^{H \times W}$ | Normalized mask - Baseline | $224 \times 224$ |
| $m_j$ | $[0,1]^{H \times W}$ | Normalized mask cho **component** $j$ | Multi-component, $j=1..K$ |
| $\mathbf{x}_{\text{masked}}$ | $\mathbb{R}^{3 \times H \times W}$ | Masked image: $\mathbf{x} \odot m$ | Element-wise product |
| $\odot$ | - | Element-wise multiplication (Hadamard product) | Pixel-wise nhÃ¢n |

#### Learnable Parameters (OptiCAM Baseline)
| KÃ½ hiá»‡u | Dimension | Ã nghÄ©a | Giáº£i thÃ­ch |
|---------|-----------|---------|-----------|
| $\mathbf{u}$ | $\mathbb{R}^{K_\ell}$ | Raw learnable weights (pre-softmax) | 2048 weights cho ResNet50 |
| $\mathbf{w}$ | $\mathbb{R}^{K_\ell}$ | Normalized weights: $\text{softmax}(\mathbf{u})$ | Tá»•ng = 1, all â‰¥ 0 |
| $w_k$ hoáº·c $w^c_k$ | scalar | Weight cho channel $k$ (class $c$) | $w_k \in [0,1]$, $\sum w_k = 1$ |

#### Learnable Parameters (Multi-Component)

**âš ï¸ LÆ¯U Ã KÃ HIá»†U:** Trong Multi-Component, dÃ¹ng **j** cho component index, **k** cho channel index Ä‘á»ƒ trÃ¡nh nháº§m láº«n.

| KÃ½ hiá»‡u | Dimension | Ã nghÄ©a | Giáº£i thÃ­ch |
|---------|-----------|---------|-----------|
| $K$ | scalar | Number of components | ThÆ°á»ng $K = 3$ |
| $\mathbf{U}$ hoáº·c $\mathbf{U}_{\text{raw}}$ | $\mathbb{R}^{K \times K_\ell}$ | Raw weights cho K components | $3 \times 2048$ cho K=3 |
| $\mathbf{u}_j$ | $\mathbb{R}^{K_\ell}$ | Weights cho **component** $j$: $\mathbf{U}[j, :]$ | Row $j$ cá»§a matrix $\mathbf{U}$, vá»›i $j=1..K$ |
| $w_{j,k}$ | scalar | Normalized weight: **component** $j$, **channel** $k$ | $\sum_{k=1}^{K_\ell} w_{j,k} = 1$ (sum over channels) |
| $\boldsymbol{\beta}$ | $\mathbb{R}^K$ | Component importance weights | $\beta_j \in [0,1]$, $\sum_{j=1}^K \beta_j = 1$ |
| $\boldsymbol{\beta}_{\text{raw}}$ | $\mathbb{R}^K$ | Raw beta (pre-softmax) | Learnable parameters |

#### Scores vÃ  Probabilities

**âš ï¸ LÆ¯U Ã:** $c$ = class index, $j$ = component index (Multi-Component).

| KÃ½ hiá»‡u | Dimension | Ã nghÄ©a | Giáº£i thÃ­ch |
|---------|-----------|---------|-----------|
| $y_c$ | scalar | Logit cho **class** $c$ | Raw output, unbounded: $y_c \in (-\infty, +\infty)$ |
| $p_c$ | scalar | Probability cho **class** $c$ | $p_c = \frac{\exp(y_c)}{\sum_{c'} \exp(y_{c'})} \in [0,1]$ |
| $p_{\text{orig}}$ | scalar | Original image probability | $p_c$ khi input = $\mathbf{x}$ |
| $p_{\text{masked}}$ | scalar | Masked image probability - Baseline | $p_c$ khi input = $\mathbf{x} \odot m$ |
| $p_j$ | scalar | **Component** $j$ probability | Multi-component: $p_c$ cho $\mathbf{x} \odot m_j$, $j=1..K$ |
| $p_{\text{combined}}$ | scalar | Combined mask probability | $p_c$ cho $\mathbf{x} \odot m_{\text{combined}}$ |

#### Operations
| KÃ½ hiá»‡u | Ã nghÄ©a | Giáº£i thÃ­ch |
|---------|---------|-----------|
| $h(\cdot)$ | Activation function | ThÆ°á»ng lÃ  ReLU: $h(z) = \max(0, z)$ hoáº·c Identity: $h(z) = z$ |
| $n(\cdot)$ | Normalization | Min-max normalization vá» $[0,1]$: $n(z) = \frac{z - \min(z)}{\max(z) - \min(z)}$ |
| $\text{up}(\cdot)$ | Upsample | Bilinear interpolation tá»« $h_\ell \times w_\ell$ Ä‘áº¿n $H \times W$ |
| $g_c(\cdot)$ | Score extraction | $g_c(\mathbf{y}) = y_c$ (láº¥y logit class $c$) hoáº·c softmax |
| $\text{softmax}(\mathbf{z})_i$ | Softmax function | $\frac{\exp(z_i)}{\sum_j \exp(z_j)}$ - normalize thÃ nh probability distribution |
| $\text{clamp}(z, a, b)$ | Clipping | $\min(\max(z, a), b)$ - giá»›i háº¡n giÃ¡ trá»‹ trong $[a, b]$ |

#### Loss Functions
| KÃ½ hiá»‡u | Dimension | Ã nghÄ©a | Giáº£i thÃ­ch |
|---------|-----------|---------|-----------|
| $\mathcal{L}$ | scalar | Total loss | Objective function Ä‘á»ƒ minimize |
| $\mathcal{L}_{\text{fidelity}}$ | scalar | Fidelity loss | Báº£o toÃ n confidence: $(p_{\text{masked}} - p_{\text{orig}})^2$ |
| $\mathcal{L}_{\text{consistency}}$ | scalar | Consistency loss | Constraint: $(\sum \beta_j p_j - p_{\text{orig}})^2$ |
| $F^c_\ell(\mathbf{x}; \mathbf{u})$ | scalar | OptiCAM objective | Score cá»§a masked image (maximize) |
| $\lambda$ hoáº·c $\lambda_t$ | scalar | Lambda weight | Balance fidelity vs consistency, $\lambda_t$ cÃ³ scheduling |

#### Optimization
| KÃ½ hiá»‡u | Ã nghÄ©a | Giáº£i thÃ­ch |
|---------|---------|-----------|
| $\eta$ | Learning rate | Adam optimizer step size, thÆ°á»ng $\eta = 0.001$ |
| $T$ | Max iterations | Sá»‘ iterations optimize, thÆ°á»ng $T = 100$ |
| $t$ | Current iteration | $t \in \{0, 1, ..., T-1\}$ |
| $B$ | Batch size | Sá»‘ images xá»­ lÃ½ cÃ¹ng lÃºc (training time) |
| $N$ | Number of samples | Tá»•ng sá»‘ samples Ä‘Ã¡nh giÃ¡ (evaluation time), e.g., $N = 68$ |
| $\nabla_{\mathbf{u}} \mathcal{L}$ | Gradient | $\frac{\partial \mathcal{L}}{\partial \mathbf{u}}$ - gradient cá»§a loss theo weights |

#### Batch Notation
| KÃ½ hiá»‡u | Dimension | Ã nghÄ©a | Giáº£i thÃ­ch |
|---------|-----------|---------|-----------|
| $B$ | scalar | Batch size | Sá»‘ images trong 1 batch |
| $\mathbf{x}_i$ | $\mathbb{R}^{3 \times H \times W}$ | Image thá»© $i$ trong batch | $i \in \{1, ..., B\}$ |
| $\mathbb{E}[\cdot]$ | - | Expectation (trung bÃ¬nh) | $\mathbb{E}[z_i] = \frac{1}{B}\sum_{i=1}^B z_i$ |

---

### ğŸ“Œ 0.1.5. INDEX CONVENTIONS - QUY Æ¯á»šC KÃ HIá»†U (CRITICAL)

**âš ï¸ Äá»‚ TRÃNH NHáº¦M LáºªN, FILE NÃ€Y TUÃ‚N THá»¦ QUY Æ¯á»šC SAU:**

| Index | Ã nghÄ©a | Range | VÃ­ dá»¥ sá»­ dá»¥ng | Äá»c lÃ  |
|-------|---------|-------|---------------|--------|
| **$c$** | **Class** index | $c = 1..C$ | $y_c$ (logit), $p_c$ (probability) | "class c" |
| **$k$** | **Channel** index (feature maps) | $k = 1..K_\ell$ | $A^k_\ell$ (feature channel $k$), $w_{j,k}$ (weight for channel $k$) | "channel k" |
| **$j$** | **Component** index (Multi-Component) | $j = 1..K$ | $S_\ell^{(j)}$ (saliency $j$), $m_j$ (mask $j$), $p_j$ (prob $j$), $\beta_j$ (weight $j$) | "component j" |
| **$i$** | **Batch/sample** index | $i = 1..B$ | $\mathbf{x}_i$ (image $i$), $p_{\text{orig},i}$ | "sample i" |
| **$\ell$** | **Layer** index | - | $A^k_\ell$ (features táº¡i layer $\ell$) | "layer ell" |

**CÃ´ng Thá»©c Quan Trá»ng vá»›i KÃ½ Hiá»‡u ÄÃºng:**

1. **Saliency map (Multi-Component):**
   $$S_\ell^{(j)} = \sum_{k=1}^{K_\ell} w_{j,k} \cdot A^k_\ell$$
   - $j$: component index (which component), $k$: channel index (which feature channel)

2. **Consistency Constraint:**
   $$\sum_{j=1}^{K} \beta_j \cdot p_j \approx p_{\text{orig}}$$
   - $j$: component index, $\beta_j$: importance cá»§a component $j$, $p_j$: prob cá»§a component $j$

3. **Combined Mask:**
   $$m_{\text{combined}} = \text{clamp}\left(\sum_{j=1}^{K} \beta_j \cdot m_j, 0, 1\right)$$
   - $j$: component index, $m_j$: mask cá»§a component $j$

**LÃ½ do:**
- TrÃ¡nh xung Ä‘á»™t: $k$ Ä‘Ã£ dÃ¹ng cho channels ($K_\ell = 2048$), khÃ´ng thá»ƒ dÃ¹ng láº¡i cho components ($K = 3$)
- RÃµ rÃ ng: $w_{j,k}$ = weight cá»§a component $j$ cho channel $k$ (component $j$, channel $k$)
- Nháº¥t quÃ¡n: Baseline dÃ¹ng $k$ cho channels, Multi-Component thÃªm $j$ cho components

---

### 0.2. Thuáº­t Ngá»¯ Quan Trá»ng

#### Explainability Terms
- **Saliency Map:** Báº£n Ä‘á»“ 2D cho biáº¿t vÃ¹ng nÃ o cá»§a áº£nh quan trá»ng vá»›i dá»± Ä‘oÃ¡n. GiÃ¡ trá»‹ cao = quan trá»ng hÆ¡n.
- **Attribution:** GÃ¡n "credit" cho tá»«ng pixel vá» contribution vÃ o prediction.
- **Faithfulness:** Má»©c Ä‘á»™ saliency map pháº£n Ã¡nh Ä‘Ãºng reasoning cá»§a model. Measured by AD, AI, AG metrics.
- **CAM (Class Activation Mapping):** PhÆ°Æ¡ng phÃ¡p táº¡o saliency map tá»« feature maps cá»§a CNN.

#### OptiCAM Specific
- **Optimization-based:** Táº¡o saliency map báº±ng cÃ¡ch optimize weights, khÃ´ng chá»‰ tÃ­nh gradient.
- **Target Layer:** Layer trong CNN Ä‘á»ƒ extract features, thÆ°á»ng lÃ  layer cuá»‘i cá»§a backbone (e.g., ResNet50 layer4[-1]).
- **Channel Weighting:** Má»—i feature map channel cÃ³ weight $w_k$ cho biáº¿t importance.
- **Linear Combination:** Saliency map = tá»•ng cÃ³ trá»ng sá»‘ cá»§a feature maps: $S = \sum_k w_k A^k$.

#### Multi-Component Specific
- **Component:** Má»™t trong K masks riÃªng biá»‡t, má»—i cÃ¡i highlight má»™t semantic part.
- **Decomposition:** PhÃ¢n tÃ¡ch prediction thÃ nh K parts Ä‘á»™c láº­p.
- **Consistency Constraint:** YÃªu cáº§u toÃ¡n há»c: tá»•ng K component scores = original score.
- **Beta Weights ($\boldsymbol{\beta}$):** Importance cá»§a má»—i component, normalized vá» tá»•ng = 1.
- **Combined Mask:** Tá»•ng cÃ³ trá»ng sá»‘ cá»§a K masks: $m_{\text{combined}} = \sum_{j=1}^{K} \beta_j m_j$.

#### Loss Terms
- **Fidelity Loss:** Äo sai khÃ¡c giá»¯a masked image score vÃ  original score. Objective: preserve prediction confidence.
- **Consistency Loss:** Äo violation cá»§a decomposition constraint. Objective: ensure $\sum_{j=1}^{K} \beta_j p_j \approx p_{\text{orig}}$.
- **Soft Constraint:** Constraint Ä‘Æ°á»£c enforce qua loss term vá»›i weight $\lambda$, khÃ´ng pháº£i hard constraint (=0 exactly).

#### Probability vs Logit Space
- **Logit Space:** Raw output cá»§a network, unbounded: $y_c \in (-\infty, +\infty)$.
- **Probability Space:** Sau softmax, bounded: $p_c \in [0, 1]$, $\sum_c p_c = 1$.
- **Why Probability Space?** Multi-Component dÃ¹ng probability vÃ¬ cÃ³ tÃ­nh cháº¥t additivity (cá»™ng Ä‘Æ°á»£c), cÃ²n logit khÃ´ng.

#### Optimization Terms
- **Adam Optimizer:** Adaptive learning rate optimizer vá»›i momentum vÃ  RMSprop.
- **Gradient Ascent:** Maximize objective $F$ báº±ng cÃ¡ch Ä‘i theo hÆ°á»›ng gradient: $\mathbf{u} \gets \mathbf{u} + \eta \nabla F$.
- **Gradient Descent:** Minimize loss $\mathcal{L}$ báº±ng cÃ¡ch Ä‘i ngÆ°á»£c gradient: $\mathbf{u} \gets \mathbf{u} - \eta \nabla \mathcal{L}$.
- **Lambda Scheduling:** $\lambda_t$ giáº£m dáº§n theo iterations Ä‘á»ƒ balance dynamic giá»¯a fidelity vÃ  consistency.

---

## 1. Tá»•ng Quan vÃ  Äá»™ng CÆ¡

### 1.1. Explainability trong Deep Learning

**Váº¥n Ä‘á»:** Deep neural networks (DNNs) hoáº¡t Ä‘á»™ng nhÆ° "black boxes" - dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c nhÆ°ng khÃ³ giáº£i thÃ­ch quyáº¿t Ä‘á»‹nh.

**Má»¥c tiÃªu:** Táº¡o **saliency maps** (báº£n Ä‘á»“ Ä‘á»™ ná»•i báº­t) Ä‘á»ƒ:
- Trá»±c quan hÃ³a vÃ¹ng áº£nh nÃ o quan trá»ng vá»›i dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh
- TÄƒng Ä‘á»™ tin cáº­y (trust) trong á»©ng dá»¥ng y táº¿, tá»± Ä‘á»™ng lÃ¡i xe, an ninh
- Debug vÃ  cáº£i thiá»‡n mÃ´ hÃ¬nh

### 1.2. PhÆ°Æ¡ng PhÃ¡p Truyá»n Thá»‘ng

**Gradient-based methods (GradCAM, Guided Backprop):**
- âœ… Nhanh (chá»‰ 1-2 forward/backward passes)
- âŒ Cháº¥t lÆ°á»£ng tháº¥p: nhiá»…u, khÃ´ng sáº¯c nÃ©t, khÃ´ng tá»‘i Æ°u trá»±c tiáº¿p cho má»¥c tiÃªu faithfulness

**Perturbation-based methods (RISE, LIME):**
- âœ… Faithfulness cao (Ä‘o trá»±c tiáº¿p áº£nh hÆ°á»Ÿng lÃªn output)
- âŒ Ráº¥t cháº­m (hÃ ng nghÃ¬n forward passes)

### 1.3. OptiCAM - Giáº£i PhÃ¡p Tá»‘i Æ¯u

**Ã tÆ°á»Ÿng chÃ­nh:** Tá»‘i Æ°u hÃ³a **trá»±c tiáº¿p** mask Ä‘á»ƒ tá»‘i Ä‘a hÃ³a faithfulness thay vÃ¬ chá»‰ tÃ­nh gradient.

**Æ¯u Ä‘iá»ƒm:**
- Cháº¥t lÆ°á»£ng cao (mask sáº¯c nÃ©t, Ã­t nhiá»…u)
- Faithfulness Ä‘Æ°á»£c Ä‘áº£m báº£o bá»Ÿi hÃ m má»¥c tiÃªu

---

## 2. OptiCAM Baseline - LÃ½ Thuyáº¿t Ná»n Táº£ng

### 2.1. Kiáº¿n TrÃºc Tá»•ng Quan

```
Input Image (x) â†’ CNN Backbone â†’ Target Layer (features f) â†’ Optimization â†’ Mask (m)
                                                                â†“
                              Masked Image (x âŠ™ m) â†’ CNN â†’ Score (y_masked)
                                                                â†“
                                                     Objective: y_masked â‰ˆ y_orig
```

**CÃ¡c thÃ nh pháº§n:**

1. **Backbone CNN:** Pre-trained model (VGG, ResNet, EfficientNet)
2. **Target Layer:** Layer giá»¯a mÃ´ hÃ¬nh (vÃ­ dá»¥: `layer4[-1]` cá»§a ResNet50)
   - Output: Feature maps `f âˆˆ â„^(CÃ—H_fÃ—W_f)`
   - ResNet50 layer4: C=2048, H_f=W_f=14 (vá»›i input 224Ã—224)

3. **Learnable Weights:** `W âˆˆ â„^(CÃ—1Ã—1)` - trá»ng sá»‘ cho má»—i channel
4. **Saliency Map:** Tá»•ng cÃ³ trá»ng sá»‘ cá»§a feature channels
5. **Mask:** Saliency map Ä‘Æ°á»£c chuáº©n hÃ³a vá» [0,1] vÃ  resize vá» kÃ­ch thÆ°á»›c input

### 2.2. CÃ´ng Thá»©c ToÃ¡n Há»c - OptiCAM Baseline

**Notation theo paper gá»‘c:**
- $\mathbf{x} \in \mathcal{X}$: Input image (image space $\mathcal{X}$)
- $f: \mathcal{X} \to \mathbb{R}^C$: Classifier network vá»›i $C$ classes
- $\mathbf{y} = f(\mathbf{x}) \in \mathbb{R}^C$: Logit vector
- $y_c = f(\mathbf{x})_c$: Logit cho class $c$
- Layer $\ell$ vá»›i $K_\ell$ channels: Feature maps $A^k_\ell \in \mathbb{R}^{h_\ell \times w_\ell}$ cho $k = 1, \ldots, K_\ell$
- Saliency map: $S_\ell \in \mathbb{R}^{h_\ell \times w_\ell}$

#### 2.2.1. Feature Extraction

Cho input image $\mathbf{x}$, sau khi Ä‘i qua target layer $\ell$:

$$
A^k_\ell = f^k_\ell(\mathbf{x}) \in \mathbb{R}^{h_\ell \times w_\ell} \quad \text{for } k = 1, \ldots, K_\ell
$$

**Giáº£ Ä‘á»‹nh:** Feature maps khÃ´ng Ã¢m (do ReLU non-linearities): $A^k_\ell \geq 0$.

**VÃ­ dá»¥ ResNet50 layer4:** $K_\ell = 2048$ channels, $h_\ell = w_\ell = 14$ (vá»›i input 224Ã—224).

#### 2.2.2. Saliency Map as Linear Combination

**General formula (Equation 1 trong paper):**

$$
S^c_\ell(\mathbf{x}) := h\left(\sum_k w^c_k A^k_\ell\right)
$$

**Giáº£i thÃ­ch tá»«ng thÃ nh pháº§n:**
- $S^c_\ell(\mathbf{x})$: Saliency map cho class $c$ táº¡i layer $\ell$, tÃ­nh tá»« image $\mathbf{x}$
- $\sum_k$: Tá»•ng theo táº¥t cáº£ channels $k = 1, 2, ..., K_\ell$ (vá»›i ResNet50: $K_\ell = 2048$)
- $w^c_k$: Trá»ng sá»‘ (weight) cá»§a channel $k$ cho class $c$. Cho biáº¿t channel nÃ y quan trá»ng bao nhiÃªu
- $A^k_\ell$: Feature map cá»§a channel $k$ táº¡i layer $\ell$, lÃ  tensor 2D kÃ­ch thÆ°á»›c $h_\ell \times w_\ell$
- $w^c_k A^k_\ell$: NhÃ¢n scalar $w^c_k$ vá»›i má»—i element cá»§a tensor $A^k_\ell$ (scalar multiplication)
- $\sum_k w^c_k A^k_\ell$: Weighted sum - cá»™ng táº¥t cáº£ $K_\ell$ feature maps Ä‘Ã£ scale, káº¿t quáº£ lÃ  1 tensor 2D duy nháº¥t
- $h(\cdot)$: Activation function - thÆ°á»ng lÃ  ReLU: $h(z) = \max(0, z)$ Ä‘á»ƒ loáº¡i bá» giÃ¡ trá»‹ Ã¢m, hoáº·c identity: $h(z) = z$

**OptiCAM formulation (Equation 8 trong paper):**

Sá»­ dá»¥ng **softmax normalization** nhÆ° Score-CAM:

$$
w_k := \text{softmax}(\mathbf{u})_k = \frac{\exp(u_k)}{\sum_{k'=1}^{K_\ell} \exp(u_{k'})}
$$

**Giáº£i thÃ­ch tá»«ng thÃ nh pháº§n:**
- $\mathbf{u} \in \mathbb{R}^{K_\ell}$: Vector chá»©a $K_\ell$ giÃ¡ trá»‹ raw (chÆ°a normalize), Ä‘Ã¢y lÃ  biáº¿n cáº§n optimize
- $u_k$: Pháº§n tá»­ thá»© $k$ cá»§a vector $\mathbf{u}$, cÃ³ thá»ƒ lÃ  sá»‘ báº¥t ká»³ (Ã¢m, dÆ°Æ¡ng, lá»›n, nhá»)
- $\exp(u_k)$: Exponential cá»§a $u_k$, luÃ´n dÆ°Æ¡ng ($> 0$) dÃ¹ $u_k$ Ã¢m hay dÆ°Æ¡ng
- $\sum_{k'=1}^{K_\ell} \exp(u_{k'})$: Tá»•ng exponential cá»§a táº¥t cáº£ $K_\ell$ elements - lÃ  háº±ng sá»‘ normalization
- $w_k$: Weight sau softmax, luÃ´n trong khoáº£ng $(0, 1)$ vÃ  $\sum_{k=1}^{K_\ell} w_k = 1$ (probability distribution)
- **Táº¡i sao softmax?** Äáº£m báº£o weights non-negative, normalized, vÃ  cÃ³ tÃ­nh cháº¥t "competition" (channels pháº£i cáº¡nh tranh Ä‘á»ƒ cÃ³ weight cao)

**Saliency map (Equation 8):**

$$
S_\ell(\mathbf{x}; \mathbf{u}) := h\left(\sum_k \text{softmax}(\mathbf{u})_k \cdot A^k_\ell\right)
$$

**Giáº£i thÃ­ch:**
- $S_\ell(\mathbf{x}; \mathbf{u})$: Saliency map lÃ  **hÃ m cá»§a** $\mathbf{u}$ (khÃ´ng pháº£i háº±ng sá»‘). Khi thay Ä‘á»•i $\mathbf{u}$, saliency map cÅ©ng thay Ä‘á»•i
- $\text{softmax}(\mathbf{u})_k$: TÃ­nh weight $w_k$ tá»« raw parameter $u_k$ qua softmax
- $\cdot$ (dáº¥u cháº¥m): NhÃ¢n scalar vá»›i tensor (giá»‘ng $w^c_k A^k_\ell$ á»Ÿ trÃªn)
- **Ã nghÄ©a:** ÄÃ¢y chÃ­nh lÃ  Equation 1 nhÆ°ng vá»›i weights $w_k$ Ä‘Æ°á»£c tÃ­nh tá»« learnable parameters $\mathbf{u}$ qua softmax

**LÃ½ do softmax:** 
- Chá»‰ xÃ©t positive contributions (convex combination vá»›i weights â‰¥ 0)
- Competition giá»¯a cÃ¡c channels â†’ attend to few important feature maps (khÃ´ng pháº£i táº¥t cáº£ channels Ä‘á»u quan trá»ng)
- TrÃ¡nh saliency map phá»§ toÃ n bá»™ áº£nh (náº¿u táº¥t cáº£ weights cao â†’ khÃ´ng discriminative)
- Differentiable â†’ cÃ³ thá»ƒ optimize báº±ng gradient descent

#### 2.2.3. Normalization Function

**Normalization to [0,1] (Equation 4 trong paper):**

$$
n(A) := \frac{A - \min A}{\max A - \min A}
$$

**Giáº£i thÃ­ch tá»«ng thÃ nh pháº§n:**
- $A$: Input tensor (saliency map), cÃ³ thá»ƒ cÃ³ giÃ¡ trá»‹ báº¥t ká»³
- $\min A$: GiÃ¡ trá»‹ nhá» nháº¥t trong tensor $A$ (scalar)
- $\max A$: GiÃ¡ trá»‹ lá»›n nháº¥t trong tensor $A$ (scalar)
- $A - \min A$: Shift táº¥t cáº£ giÃ¡ trá»‹ Ä‘á»ƒ minimum = 0 (tensor cÃ¹ng shape vá»›i $A$)
- $\max A - \min A$: Range (khoáº£ng) cá»§a giÃ¡ trá»‹ trong $A$ (scalar)
- $\frac{A - \min A}{\max A - \min A}$: Scale vá» khoáº£ng $[0, 1]$ - min â†’ 0, max â†’ 1
- **Convention Ä‘áº·c biá»‡t:** Náº¿u $A = \mathbf{0}$ (all zeros) â†’ $\max A = \min A = 0$ â†’ define $n(\mathbf{0}) := \mathbf{0}$ Ä‘á»ƒ trÃ¡nh chia cho 0

**Ãp dá»¥ng:**

$$
S_{\text{norm}} = n\left(\text{up}(S_\ell(\mathbf{x}; \mathbf{u}))\right)
$$

**Giáº£i thÃ­ch:**
- $S_\ell(\mathbf{x}; \mathbf{u})$: Saliency map á»Ÿ resolution $h_\ell \times w_\ell$ (e.g., $14 \times 14$)
- $\text{up}(\cdot)$: Upsample function - bilinear interpolation Ä‘á»ƒ scale lÃªn input size $H \times W$ (e.g., $224 \times 224$)
- $\text{up}(S_\ell)$: Saliency map sau upsample, cÃ¹ng size vá»›i input image
- $n(\cdot)$: Normalize vá» $[0, 1]$ - sau Ä‘Ã³ cÃ³ thá»ƒ dÃ¹ng lÃ m mask
- **Thá»© tá»±:** Upsample **trÆ°á»›c**, normalize **sau** (Ä‘á»ƒ giá»¯ tÃ­nh spatial continuity)

á» Ä‘Ã¢y $\text{up}(\cdot)$ lÃ  bilinear upsampling lÃªn resolution cá»§a $\mathbf{x}$.

#### 2.2.4. Masked Image

**Element-wise multiplication (Hadamard product):**

$$
\mathbf{x}_{\text{masked}} = \mathbf{x} \odot n(\text{up}(S_\ell(\mathbf{x}; \mathbf{u})))
$$

**Giáº£i thÃ­ch tá»«ng thÃ nh pháº§n:**
- $\mathbf{x}$: Input image RGB, tensor shape $3 \times H \times W$ (3 channels mÃ u)
- $S_\ell(\mathbf{x}; \mathbf{u})$: Saliency map táº¡i layer $\ell$ (shape $h_\ell \times w_\ell$)
- $\text{up}(S_\ell)$: Upsample lÃªn shape $H \times W$ Ä‘á»ƒ match vá»›i image
- $n(\text{up}(S_\ell))$: Normalize vá» $[0, 1]$ - Ä‘Ã¢y lÃ  mask $m \in [0,1]^{H \times W}$
- $\odot$: Element-wise multiplication (Hadamard product) - nhÃ¢n tá»«ng pixel má»™t
- **Broadcasting:** Mask 2D shape $(H, W)$ Ä‘Æ°á»£c broadcast thÃ nh $(3, H, W)$ Ä‘á»ƒ nhÃ¢n vá»›i RGB image
- $\mathbf{x}_{\text{masked}}$: Masked image - pixels á»Ÿ vÃ¹ng mask=1 giá»¯ nguyÃªn, mask=0 bá»‹ zero out, maskâˆˆ(0,1) bá»‹ darken

**LÆ°u Ã½:** Saliency map $\in \mathbb{R}^{H \times W}$ Ä‘Æ°á»£c broadcast qua 3 channels RGB cá»§a $\mathbf{x} \in \mathbb{R}^{3 \times H \times W}$.

**VÃ­ dá»¥ cá»¥ thá»ƒ:**
```python
# x.shape = (3, 224, 224) - RGB image
# mask.shape = (224, 224) - saliency map normalized to [0,1]
# Broadcasting: mask â†’ (1, 224, 224) â†’ (3, 224, 224)
x_masked = x * mask  # Element-wise: x_masked[c, i, j] = x[c, i, j] Ã— mask[i, j]
```

### 2.3. HÃ m Má»¥c TiÃªu OptiCAM Baseline

#### 2.3.1. Objective Function (Equations 9-10 trong paper)

**Optimization problem (Equation 9):**

$$
\mathbf{u}^* := \arg\max_{\mathbf{u}} F^c_\ell(\mathbf{x}; \mathbf{u})
$$

**Giáº£i thÃ­ch tá»«ng thÃ nh pháº§n:**
- $\mathbf{u}$: Biáº¿n tá»‘i Æ°u - vector weights $\in \mathbb{R}^{K_\ell}$ (e.g., 2048 dimensions cho ResNet50)
- $F^c_\ell(\mathbf{x}; \mathbf{u})$: Objective function (hÃ m má»¥c tiÃªu) - scalar value Ä‘o "tá»‘t" cá»§a $\mathbf{u}$
- $\arg\max_{\mathbf{u}}$: TÃ¬m giÃ¡ trá»‹ $\mathbf{u}$ Ä‘á»ƒ **maximize** (lÃ m lá»›n nháº¥t) $F^c_\ell$
- $\mathbf{u}^*$: Optimal weights - giÃ¡ trá»‹ tá»‘t nháº¥t cá»§a $\mathbf{u}$ sau optimization
- **Ã nghÄ©a:** ÄÃ¢y lÃ  **optimization problem**, khÃ´ng pháº£i closed-form solution. Pháº£i dÃ¹ng gradient ascent/descent

**Objective function (Equation 10):**

$$
F^c_\ell(\mathbf{x}; \mathbf{u}) := g_c\left(f\left(\mathbf{x} \odot n(\text{up}(S_\ell(\mathbf{x}; \mathbf{u})))\right)\right)
$$

**Giáº£i thÃ­ch tá»«ng thÃ nh pháº§n (tá»« trong ra ngoÃ i):**
1. $S_\ell(\mathbf{x}; \mathbf{u})$: Saliency map tá»« weights $\mathbf{u}$ (Equation 8) - shape $h_\ell \times w_\ell$
2. $\text{up}(S_\ell)$: Upsample lÃªn input resolution - shape $H \times W$
3. $n(\text{up}(S_\ell))$: Normalize vá» $[0,1]$ - Ä‘Ã¢y lÃ  mask $m$
4. $\mathbf{x} \odot n(\text{up}(S_\ell))$: Masked image - $\mathbf{x}_{\text{masked}}$ shape $3 \times H \times W$
5. $f(\cdot)$: CNN classifier (forward pass) - input: image, output: logit vector $\mathbf{y} \in \mathbb{R}^C$
6. $f(\mathbf{x}_{\text{masked}})$: Logits cá»§a masked image - vector shape $(C,)$ vá»›i $C$ classes
7. $g_c(\cdot)$: Selector function - extract logit cá»§a class $c$ má»¥c tiÃªu
8. $g_c(f(\mathbf{x}_{\text{masked}}))$: Scalar value - logit cá»§a class $c$ cho masked image
9. $F^c_\ell(\mathbf{x}; \mathbf{u})$: Final objective - scalar Ä‘á»ƒ maximize

Vá»›i:
- $S_\ell(\mathbf{x}; \mathbf{u})$: Saliency map (Equation 8)
- $n(\cdot)$: Normalization function (Equation 4)
- $\text{up}(\cdot)$: Upsampling to input resolution
- $g_c(\mathbf{y})$: Selector function trÃªn logit vector

**Selector function $g_c$ (default):**

$$
g_c(\mathbf{y}) := y_c
$$

**Giáº£i thÃ­ch:**
- $\mathbf{y} = [y_0, y_1, ..., y_{C-1}]$: Logit vector tá»« network, má»—i $y_i$ lÃ  logit cá»§a class $i$
- $g_c(\mathbf{y}) = y_c$: Láº¥y pháº§n tá»­ thá»© $c$ cá»§a vector (indexing operation)
- **VÃ­ dá»¥:** Vá»›i dog class ($c=1$) vÃ  $\mathbf{y} = [-2.1, 8.5, 3.2, ...]$ â†’ $g_c(\mathbf{y}) = 8.5$

Tá»©c lÃ  chá»n logit cá»§a class $c$ má»¥c tiÃªu.

**Ã nghÄ©a:** TÃ¬m weights $\mathbf{u}^*$ Ä‘á»ƒ **maximize logit** cá»§a masked image cho class $c$. Logit cao = model confident ráº±ng masked image váº«n thuá»™c class $c$ = mask giá»¯ Ä‘Æ°á»£c nhá»¯ng vÃ¹ng quan trá»ng.

#### 2.3.2. Final Saliency Map (Equation 11 trong paper)

$$
S^c_\ell(\mathbf{x}) := S_\ell(\mathbf{x}; \mathbf{u}^*) = S_\ell\left(\mathbf{x}; \arg\max_{\mathbf{u}} F^c_\ell(\mathbf{x}; \mathbf{u})\right)
$$

**Giáº£i thÃ­ch:**
- $\mathbf{u}^*$: Optimal weights tá»« Equation 9 (sau khi cháº¡y optimization ~100 iterations)
- $S_\ell(\mathbf{x}; \mathbf{u}^*)$: Saliency map táº¡o tá»« optimal weights
- $S^c_\ell(\mathbf{x})$: Final saliency map cho class $c$ - Ä‘Ã¢y lÃ  output cuá»‘i cÃ¹ng cá»§a OptiCAM
- **Ã nghÄ©a:** Saliency map "tá»‘t nháº¥t" sau khi optimize - highlight vÃ¹ng quan trá»ng nháº¥t cho class $c$

#### 2.3.3. Táº¡i Sao Maximize Logit?

**Äá»™ng cÆ¡ tá»« Score-CAM:**

Score-CAM Ä‘á»‹nh nghÄ©a weights dá»±a trÃªn "increase in confidence" (Equation 3 trong paper):

$$
u^c_k := f(\mathbf{x} \odot n(\text{up}(A^k_\ell)))_c - f(\mathbf{x}_b)_c
$$

**Giáº£i thÃ­ch:**
- $A^k_\ell$: Feature map cá»§a channel $k$ riÃªng láº» (1 trong 2048 channels)
- $\mathbf{x} \odot n(\text{up}(A^k_\ell))$: Masked image chá»‰ dÃ¹ng channel $k$ lÃ m mask
- $f(\mathbf{x} \odot n(\text{up}(A^k_\ell)))_c$: Logit cá»§a class $c$ cho masked image nÃ y
- $\mathbf{x}_b$: Baseline image (thÆ°á»ng $\mathbf{x}_b = \mathbf{0}$ - all black)
- $f(\mathbf{x}_b)_c$: Logit baseline (thÆ°á»ng ráº¥t tháº¥p vÃ¬ áº£nh Ä‘en)
- $u^c_k$: Weight cho channel $k$ = increase in logit = channel nÃ y boost confidence bao nhiÃªu
- **Score-CAM:** TÃ­nh $u^c_k$ cho **tá»«ng channel riÃªng láº»** (2048 forward passes!)

Vá»›i $\mathbf{x}_b$ lÃ  baseline image (thÆ°á»ng lÃ  $\mathbf{0}$).

**OptiCAM generalization:**

Thay vÃ¬ Ä‘Ã¡nh giÃ¡ tá»«ng feature map riÃªng láº», OptiCAM optimize **linear combination**:

$$
F(\mathbf{w}) := f\left(\mathbf{x} \odot n\left(\text{up}\left(\sum_k w_k A^k_\ell\right)\right)\right)_c
$$

**Giáº£i thÃ­ch:**
- $\sum_k w_k A^k_\ell$: Linear combination cá»§a **Táº¤T Cáº¢** channels cÃ¹ng lÃºc (khÃ´ng pháº£i tá»«ng channel riÃªng)
- $F(\mathbf{w})$: Logit khi dÃ¹ng combination nÃ y lÃ m mask
- **OptiCAM:** Optimize $\mathbf{w}$ (qua $\mathbf{u}$) Ä‘á»ƒ maximize $F$ - tÃ¬m **best combination** trá»±c tiáº¿p
- **Lá»£i Ã­ch:** KhÃ´ng cáº§n evaluate tá»«ng channel (2048 forwards) â†’ chá»‰ cáº§n ~100 iterations vá»›i gradient descent

**Score-CAM nhÆ° numerical gradient:**

Score-CAM weights cÃ³ thá»ƒ viáº¿t láº¡i nhÆ° (giáº£ sá»­ $\mathbf{x}_b = \mathbf{0}$):

$$
u^c_k = \frac{F(\mathbf{w}_0 + \delta \mathbf{e}_k) - F(\mathbf{w}_0)}{\delta}
$$

Vá»›i $\mathbf{w}_0 = \mathbf{0}$, $\delta = 1$, $\mathbf{e}_k$ lÃ  standard basis vector thá»© $k$.

**OptiCAM nhÆ° analytical gradient:**

Thay vÃ¬ numerical approximation, OptiCAM dÃ¹ng **backpropagation** Ä‘á»ƒ tÃ­nh $\nabla_{\mathbf{u}} F^c_\ell$ vÃ  optimize iteratively vá»›i gradient descent.

**Lá»£i Ã­ch:**
1. **Principled optimization:** Converge Ä‘áº¿n local maximum cá»§a $F^c_\ell$
2. **Efficient:** 1 backward pass thay vÃ¬ $K_\ell$ forward passes (náº¿u iterations < channels)
3. **Flexible:** CÃ³ thá»ƒ dÃ¹ng advanced optimizers (Adam, momentum, etc.)

#### 2.3.4. Táº¡i Sao MSE Loss (Trong Multi-Component)?

**LÆ°u Ã½:** OptiCAM baseline **khÃ´ng cÃ³ explicit loss function** - chá»‰ maximize objective $F^c_\ell$.

Tuy nhiÃªn trong Multi-Component OptiCAM, chÃºng ta cáº§n **constraint** nÃªn dÃ¹ng loss:

| Loss Type | CÃ´ng Thá»©c | Gradient | Æ¯u Äiá»ƒm | NhÆ°á»£c Äiá»ƒm |
|-----------|-----------|----------|---------|------------|
| **L1 (MAE)** | $\|y - \hat{y}\|$ | $\text{sign}(y - \hat{y})$ | Robust vá»›i outliers | Gradient khÃ´ng liÃªn tá»¥c táº¡i 0 |
| **L2 (MSE)** | $(y - \hat{y})^2$ | $2(y - \hat{y})$ | Smooth gradient, á»•n Ä‘á»‹nh | Nháº¡y cáº£m vá»›i outliers |
| **Huber** | Piecewise L1/L2 | Piecewise | Balanced | Phá»©c táº¡p hÆ¡n |

**LÃ½ do chá»n MSE (L2) cho Multi-Component:**
1. **Smooth gradients:** $\nabla \mathcal{L} = 2(y - \hat{y}) \cdot \nabla \hat{y}$ - liÃªn tá»¥c kháº¯p nÆ¡i
2. **Stable optimization:** Adam optimizer há»™i tá»¥ tá»‘t vá»›i squared error
3. **Penalty scaling:** Sai sá»‘ lá»›n bá»‹ pháº¡t náº·ng hÆ¡n (quadratic) â†’ Æ°u tiÃªn giáº£m violation lá»›n
4. **Standard practice:** Äa sá»‘ papers vá» optimization-based explanations dÃ¹ng MSE

### 2.4. Optimization Algorithm

**Algorithm: Gradient Ascent vá»›i Adam Optimizer**

OptiCAM sá»­ dá»¥ng **gradient ascent** Ä‘á»ƒ maximize $F^c_\ell(\mathbf{x}; \mathbf{u})$ (Equation 9).

```
Input: Image x, network f, layer â„“, class c, iterations T
Extract: Feature maps {A^k_â„“}_{k=1}^{K_â„“} from layer â„“

Initialize: u ~ N(0, 0.01)  [random initialization]

For t = 1 to T:
    1. w_k = softmax(u)_k                    [Equation 8: weights]
    2. S = h(Î£_k w_k Â· A^k_â„“)                [Equation 8: saliency map]
    3. S_up = up(S)                          [upsample to input size]
    4. S_norm = n(S_up)                      [Equation 4: normalize to [0,1]]
    5. x_masked = x âŠ™ S_norm                 [masked image]
    6. y_masked = f(x_masked)_c              [forward pass â†’ logit]
    7. F = g_c(y_masked) = y_masked          [Equation 10: objective]
    8. u â† Adam_update(u, âˆ‡_u F)            [gradient ascent]

Return: S^c_â„“(x) = S_â„“(x; u*)               [Equation 11: final saliency map]
```

**Key Points:**

1. **Maximize objective:** $\max_{\mathbf{u}} F^c_\ell$ (khÃ´ng pháº£i minimize loss)
2. **Variable:** $\mathbf{u} \in \mathbb{R}^{K_\ell}$ - chá»‰ $K_\ell$ parameters (2048 cho ResNet50)
3. **Fixed:** Feature maps $\{A^k_\ell\}$ vÃ  network $f$ - khÃ´ng train
4. **Differentiable path:** $\mathbf{u} \to S_\ell \to \mathbf{x}_{\text{masked}} \to F^c_\ell$ - toÃ n bá»™ differentiable

**Hyperparameters (tá»« paper + implementation):**
- Learning rate: `lr = 0.01` (OptiCAM baseline paper)
- Iterations: `T = 100` (max_iter)
- Optimizer: Adam vá»›i $\beta_1 = 0.9$, $\beta_2 = 0.999$, $\epsilon = 10^{-8}$
- Activation $h$: Identity hoáº·c ReLU (paper dÃ¹ng ReLU nhÆ° Grad-CAM)

---

## 3. Multi-Component OptiCAM - Má»Ÿ Rá»™ng

### 3.1. Äá»™ng CÆ¡

**Váº¥n Ä‘á» cá»§a OptiCAM baseline:**
- Chá»‰ táº¡o **1 mask duy nháº¥t** â†’ khÃ´ng phÃ¢n tÃ­ch Ä‘Æ°á»£c cÃ¡c factors Ä‘á»™c láº­p
- VÃ­ dá»¥: áº¢nh chÃ³ - khÃ´ng tÃ¡ch Ä‘Æ°á»£c "Ä‘áº§u chÃ³" vs "Ä‘uÃ´i chÃ³" vs "background"

**Má»¥c tiÃªu Multi-Component:**
- Táº¡o **K masks riÃªng biá»‡t** (`mask_1, mask_2, ..., mask_K`)
- Má»—i mask táº­p trung vÃ o má»™t "semantic component" khÃ¡c nhau
- **Constraint:** Tá»•ng áº£nh hÆ°á»Ÿng cá»§a K masks = áº£nh hÆ°á»Ÿng áº£nh gá»‘c

### 3.2. Kiáº¿n TrÃºc Multi-Component

```
Input (x) â†’ Features (f) âˆˆ â„^(CÃ—H_fÃ—W_f)
                â†“
         [K Learnable Weights]
         U_raw âˆˆ â„^(KÃ—CÃ—1Ã—1)
                â†“
         W = softmax(U_raw, dim=C) âˆˆ â„^(KÃ—CÃ—1Ã—1)
                â†“
         [K Component Masks]
         mask_j = Î£(w_{j,k} Â· f_k) for j=1..K (j=component, k=channel)
                â†“
         [K Masked Images]
         x_j = mask_j âŠ™ x for j=1..K
                â†“
         [K Component Scores]
         y_j = CNN(x_j) for j=1..K
                â†“
         [Combined Mask]
         mask_combined = Î£(Î²_j Â· mask_j)
         x_combined = mask_combined âŠ™ x
         y_combined = CNN(x_combined)
                â†“
         [Two Objectives]
         L_fidelity: y_combined â‰ˆ y_orig
         L_consistency: Î£(Î²_j Â· y_j) â‰ˆ y_orig
```

### 3.3. CÃ´ng Thá»©c ToÃ¡n Há»c - Multi-Component

**LÆ°u Ã½ vá» kÃ½ hiá»‡u:** Multi-Component OptiCAM lÃ  extension cá»§a OptiCAM baseline, nÃªn giá»¯ nguyÃªn kÃ½ hiá»‡u gá»‘c khi cÃ³ thá»ƒ. Chá»‰ thÃªm subscript/superscript $j$ cho components.

#### 3.3.1. Learnable Parameters

**Channel weights cho K components (má»Ÿ rá»™ng tá»« Equation 8):**

Thay vÃ¬ 1 vector $\mathbf{u} \in \mathbb{R}^{K_\ell}$, giá» cÃ³ $K$ vectors:

$$
\mathbf{U} \in \mathbb{R}^{K \times K_\ell}
$$

**Giáº£i thÃ­ch:**
- Baseline: $\mathbf{u} \in \mathbb{R}^{K_\ell}$ - 1 vector vá»›i $K_\ell$ elements (e.g., 2048)
- Multi-Component: $\mathbf{U} \in \mathbb{R}^{K \times K_\ell}$ - **matrix** vá»›i $K$ rows, má»—i row lÃ  1 vector
- $K$: Sá»‘ components (thÆ°á»ng $K = 3$)
- $K_\ell$: Sá»‘ channels (e.g., 2048 cho ResNet50 layer4)
- **VÃ­ dá»¥:** $K=3$, $K_\ell=2048$ â†’ $\mathbf{U}$ lÃ  matrix $3 \times 2048$ = 6,144 parameters

Vá»›i $\mathbf{U}[j, :] = \mathbf{u}_j$ lÃ  weights cho component $j$.

**Giáº£i thÃ­ch indexing:**
- $\mathbf{U}[j, :]$: Row thá»© $j$ cá»§a matrix $\mathbf{U}$ (Python/numpy notation)
- $\mathbf{u}_j = \mathbf{U}[j, :] \in \mathbb{R}^{K_\ell}$: Vector weights cho component $j$
- $\mathbf{u}_1, \mathbf{u}_2, \mathbf{u}_3$: 3 vectors riÃªng biá»‡t, má»—i cÃ¡i cÃ³ 2048 elements
- **Ã nghÄ©a:** Má»—i component cÃ³ **bá»™ weights riÃªng** Ä‘á»ƒ combine 2048 channels theo cÃ¡ch khÃ¡c nhau

**Component importance weights (beta) - Má»šI:**

$$
\boldsymbol{\beta}_{\text{raw}} \in \mathbb{R}^K, \quad \boldsymbol{\beta} = \text{softmax}(\boldsymbol{\beta}_{\text{raw}}) = \frac{\exp(\boldsymbol{\beta}_{\text{raw}})}{\sum_{j'=1}^{K} \exp(\beta_{\text{raw}, j'})}
$$

**Giáº£i thÃ­ch tá»«ng thÃ nh pháº§n:**
- $\boldsymbol{\beta}_{\text{raw}} \in \mathbb{R}^K$: Vector chá»©a $K$ raw values (pre-softmax) - learnable parameters
- $\beta_{\text{raw}, j}$: Element thá»© $j$ cá»§a vector $\boldsymbol{\beta}_{\text{raw}}$
- $\exp(\beta_{\text{raw}, j})$: Exponential cá»§a element thá»© $j$
- $\sum_{j'=1}^{K} \exp(\beta_{\text{raw}, j'})$: Tá»•ng exponentials cá»§a táº¥t cáº£ $K$ elements - normalization constant
- $\boldsymbol{\beta} = [\beta_1, \beta_2, ..., \beta_K]$: Normalized importance weights sau softmax
- **TÃ­nh cháº¥t:** $\beta_j \in (0, 1)$ vÃ  $\sum_{j=1}^{K} \beta_j = 1$ - lÃ  probability distribution

Vá»›i $\sum_{j=1}^{K} \beta_j = 1$ (normalized importance scores).

**LÆ°u Ã½:** $\boldsymbol{\beta}$ khÃ´ng cÃ³ trong OptiCAM baseline - Ä‘Ã¢y lÃ  thÃªm vÃ o Ä‘á»ƒ weight components.

**Ã nghÄ©a beta weights:**
- $\beta_j$ cao ($\approx 0.5$): Component $j$ quan trá»ng, contribute nhiá»u vÃ o prediction
- $\beta_j$ tháº¥p ($\approx 0.1$): Component $j$ Ã­t quan trá»ng hÆ¡n
- **VÃ­ dá»¥:** $\boldsymbol{\beta} = [0.5, 0.35, 0.15]$ â†’ component 1 quan trá»ng nháº¥t, component 3 Ã­t quan trá»ng nháº¥t

#### 3.3.2. Component Mask Creation

**Softmax normalization cho má»—i component (má»Ÿ rá»™ng Equation 8):**

$$
w_{j,k} = \text{softmax}(\mathbf{u}_j)_k = \frac{\exp(u_{j,k})}{\sum_{k'=1}^{K_\ell} \exp(u_{j,k'})} \quad \text{for } j=1..K, k=1..K_\ell
$$

**Giáº£i thÃ­ch tá»«ng thÃ nh pháº§n:**
- $w_{j,k}$: Weight cá»§a **component** $j$ cho **channel** $k$ (double subscript)
- $\mathbf{u}_j = \mathbf{U}[j, :]$: Vector weights cá»§a component $j$ (shape $K_\ell$)
- $u_{j,k}$: Element cá»§a vector $\mathbf{u}_j$ á»Ÿ vá»‹ trÃ­ $k$ - raw weight (unbounded)
- $\text{softmax}(\mathbf{u}_j)_k$: Ãp dá»¥ng softmax lÃªn **toÃ n bá»™ vector** $\mathbf{u}_j$, láº¥y element thá»© $k$
- $\sum_{k'=1}^{K_\ell}$: Tá»•ng theo táº¥t cáº£ channels (normalization constant cho component $j$)
- **TÃ­nh cháº¥t:** $w_{j,k} \in (0, 1)$ vÃ  $\sum_{k=1}^{K_\ell} w_{j,k} = 1$ **cho má»—i component $j$**
- **Ã nghÄ©a:** Má»—i component phÃ¢n phá»‘i 100% attention lÃªn 2048 channels theo cÃ¡ch riÃªng

**Loop notation:**
- "for $j=1..K$": Apply cÃ´ng thá»©c cho táº¥t cáº£ $K$ components
- "for $k=1..K_\ell$": Vá»›i má»—i component, tÃ­nh weight cho táº¥t cáº£ $K_\ell$ channels
- **Káº¿t quáº£:** Matrix $\mathbf{W} \in \mathbb{R}^{K \times K_\ell}$ chá»©a táº¥t cáº£ $w_{j,k}$

**Saliency map cho component j (theo Equation 8):**

$$
S^{(j)}_\ell(\mathbf{x}; \mathbf{u}_j) = h\left(\sum_{k=1}^{K_\ell} w_{j,k} \cdot A^k_\ell\right) \in \mathbb{R}^{h_\ell \times w_\ell}
$$

**Giáº£i thÃ­ch chi tiáº¿t tá»«ng thÃ nh pháº§n:**

1. **$S^{(j)}_\ell$** - Saliency map cá»§a component $j$ táº¡i layer $\ell$
   - Superscript $(j)$: Chá»‰ **component index** (component thá»© $j$, vá»›i $j = 1, 2, ..., K$)
   - Subscript $\ell$: Chá»‰ **layer index** (target layer, vÃ­ dá»¥: ResNet50 layer4[-1])
   - Output: Tensor 2D kÃ­ch thÆ°á»›c $h_\ell \times w_\ell$ (vÃ­ dá»¥: $14 \times 14$)
   - **Ã nghÄ©a:** ÄÃ¢y lÃ  "báº£n Ä‘á»“ táº§m quan trá»ng" cho component $j$, cho biáº¿t vÃ¹ng nÃ o cá»§a áº£nh quan trá»ng vá»›i semantic part nÃ y

2. **$(\mathbf{x}; \mathbf{u}_j)$** - Function arguments
   - $\mathbf{x}$: Input image (RGB, shape $3 \times H \times W$, vÃ­ dá»¥: $3 \times 224 \times 224$)
   - $\mathbf{u}_j$: Vector weights cá»§a component $j$ (shape $K_\ell$, vÃ­ dá»¥: 2048 elements)
   - Dáº¥u ";": PhÃ¢n biá»‡t giá»¯a input ($\mathbf{x}$) vÃ  learnable parameters ($\mathbf{u}_j$)
   - **Ã nghÄ©a:** Saliency map phá»¥ thuá»™c vÃ o cáº£ áº£nh input vÃ  weights Ä‘Æ°á»£c há»c

3. **$h(\cdot)$** - Activation function
   - ThÆ°á»ng lÃ  **ReLU**: $h(z) = \max(0, z)$ - loáº¡i bá» giÃ¡ trá»‹ Ã¢m
   - Hoáº·c **Identity**: $h(z) = z$ - giá»¯ nguyÃªn (náº¿u feature maps Ä‘Ã£ positive)
   - **Ã nghÄ©a:** Äáº£m báº£o saliency map khÃ´ng cÃ³ giÃ¡ trá»‹ Ã¢m (vÃ¬ Ã¢m khÃ´ng cÃ³ Ã½ nghÄ©a "táº§m quan trá»ng")

4. **$\sum_{k=1}^{K_\ell}$** - Summation over all channels
   - $k$: **Channel index** - cháº¡y tá»« 1 Ä‘áº¿n $K_\ell$ (vÃ­ dá»¥: $k = 1, 2, ..., 2048$)
   - $K_\ell$: Tá»•ng sá»‘ channels táº¡i layer $\ell$ (ResNet50 layer4: $K_\ell = 2048$)
   - **Ã nghÄ©a:** Tá»•ng há»£p thÃ´ng tin tá»« **Táº¤T Cáº¢** 2048 channels, má»—i channel Ä‘Ã³ng gÃ³p vá»›i trá»ng sá»‘ riÃªng

5. **$w_{j,k}$** - Normalized weight (sau softmax)
   - Subscript $j$: **Component index** - component nÃ o (vÃ­ dá»¥: component 1 = Ä‘áº§u chÃ³)
   - Subscript $k$: **Channel index** - channel nÃ o (vÃ­ dá»¥: channel 10 = edge detector)
   - **GiÃ¡ trá»‹:** $w_{j,k} \in (0, 1)$ - luÃ´n dÆ°Æ¡ng, trong khoáº£ng 0 Ä‘áº¿n 1
   - **Constraint:** $\sum_{k=1}^{K_\ell} w_{j,k} = 1$ - tá»•ng táº¥t cáº£ weights cá»§a component $j$ = 1
   - **Ã nghÄ©a:** Cho biáº¿t channel $k$ quan trá»ng **bao nhiÃªu** Ä‘á»‘i vá»›i component $j$
   - **VÃ­ dá»¥:** $w_{1,10} = 0.05$ nghÄ©a lÃ  "channel 10 Ä‘Ã³ng gÃ³p 5% vÃ o component 1 (Ä‘áº§u chÃ³)"

6. **$A^k_\ell$** - Feature map cá»§a channel $k$
   - Superscript $k$: **Channel index** - feature map cá»§a channel nÃ o
   - Subscript $\ell$: **Layer index** - tá»« layer nÃ o
   - **Shape:** $\mathbb{R}^{h_\ell \times w_\ell}$ - tensor 2D (vÃ­ dá»¥: $14 \times 14$)
   - **Nguá»“n:** Output cá»§a CNN táº¡i target layer: $A^k_\ell = f^k_\ell(\mathbf{x})$
   - **Ã nghÄ©a:** Má»—i spatial location $(i, j)$ trong $A^k_\ell$ chá»©a "activation strength" cá»§a feature detector $k$ táº¡i vá»‹ trÃ­ Ä‘Ã³
   - **VÃ­ dá»¥:** $A^{10}_{\ell}[3, 5] = 0.8$ nghÄ©a lÃ  channel 10 phÃ¡t hiá»‡n feature máº¡nh (0.8) táº¡i vá»‹ trÃ­ (3,5)

7. **$w_{j,k} \cdot A^k_\ell$** - Weighted feature map
   - **PhÃ©p toÃ¡n:** Scalar multiplication - nhÃ¢n scalar $w_{j,k}$ vá»›i má»—i element cá»§a tensor $A^k_\ell$
   - **Shape:** $\mathbb{R}^{h_\ell \times w_\ell}$ - giá»¯ nguyÃªn shape cá»§a feature map
   - **Ã nghÄ©a:** "Scale" feature map $k$ theo táº§m quan trá»ng $w_{j,k}$ Ä‘á»‘i vá»›i component $j$
   - **VÃ­ dá»¥:** Náº¿u $w_{1,10} = 0.05$ vÃ  $A^{10}_\ell[3,5] = 0.8$ â†’ weighted value = $0.05 \times 0.8 = 0.04$

8. **$\sum_{k=1}^{K_\ell} w_{j,k} \cdot A^k_\ell$** - Linear combination (weighted sum)
   - **PhÃ©p toÃ¡n:** Cá»™ng 2048 tensors 2D (má»—i tensor Ä‘Ã£ Ä‘Æ°á»£c weighted)
   - **Shape:** $\mathbb{R}^{h_\ell \times w_\ell}$ - káº¿t quáº£ lÃ  1 tensor 2D duy nháº¥t
   - **Ã nghÄ©a:** Tá»•ng há»£p thÃ´ng tin tá»« táº¥t cáº£ channels, má»—i channel Ä‘Ã³ng gÃ³p theo tá»· lá»‡ $w_{j,k}$
   - **VÃ­ dá»¥ táº¡i vá»‹ trÃ­ (3,5):**
     ```
     sum[3,5] = w_{j,1}Ã—A^1[3,5] + w_{j,2}Ã—A^2[3,5] + ... + w_{j,2048}Ã—A^2048[3,5]
              = 0.02Ã—0.5 + 0.03Ã—0.7 + ... + 0.01Ã—0.9
              = [giÃ¡ trá»‹ káº¿t há»£p tá»« 2048 channels]
     ```

**TÃ³m táº¯t Ã½ nghÄ©a toÃ n bá»™ cÃ´ng thá»©c:**

"Saliency map cá»§a component $j$ Ä‘Æ°á»£c táº¡o báº±ng cÃ¡ch:
1. Láº¥y **táº¥t cáº£ 2048 feature maps** tá»« layer4 cá»§a ResNet50
2. Má»—i feature map Ä‘Æ°á»£c **nhÃ¢n vá»›i má»™t trá»ng sá»‘** $w_{j,k}$ (Ä‘Ã£ normalize, tá»•ng = 1)
3. **Cá»™ng táº¥t cáº£** 2048 feature maps Ä‘Ã£ weighted láº¡i thÃ nh 1 map duy nháº¥t
4. Ãp dá»¥ng **activation function** $h$ (ReLU hoáº·c Identity) Ä‘á»ƒ loáº¡i bá» giÃ¡ trá»‹ Ã¢m
5. Káº¿t quáº£ lÃ  1 tensor 2D ($14 \times 14$) cho biáº¿t vÃ¹ng nÃ o quan trá»ng vá»›i component $j$"

**VÃ­ dá»¥ cá»¥ thá»ƒ vá»›i K=3 components:**
- Component 1 ($j=1$): Weights $\{w_{1,1}, w_{1,2}, ..., w_{1,2048}\}$ â†’ Saliency map táº­p trung vÃ o **Ä‘áº§u chÃ³**
- Component 2 ($j=2$): Weights $\{w_{2,1}, w_{2,2}, ..., w_{2,2048}\}$ â†’ Saliency map táº­p trung vÃ o **thÃ¢n chÃ³**
- Component 3 ($j=3$): Weights $\{w_{3,1}, w_{3,2}, ..., w_{3,2048}\}$ â†’ Saliency map táº­p trung vÃ o **background**

Má»—i component há»c **má»™t bá»™ weights riÃªng biá»‡t**, do Ä‘Ã³ táº¡o ra **3 saliency maps khÃ¡c nhau** tá»« cÃ¹ng 2048 feature maps!


**CONSTRAINT QUAN TRá»ŒNG:**

$$\sum_{k=1}^{K_\ell} w_{j,k} = 1 \quad \forall j$$

**Ã nghÄ©a:** Má»—i component phÃ¢n phá»‘i **100% attention** lÃªn 2048 channels. ÄÃ¢y lÃ  probability distribution over channels.

**Normalize vÃ  upsample:**

$$
m_j = n(\text{up}(S^{(j)}_\ell(\mathbf{x}; \mathbf{u}_j))) \in [0,1]^{H \times W}
$$

---

**Ã nghÄ©a:** Má»—i component $j$ lÃ  má»™t **linear combination riÃªng biá»‡t** cá»§a táº¥t cáº£ $K_\ell$ feature maps.

#### 3.3.2a. Chi Tiáº¿t: CÃ¡ch Táº¡o K Components

**ÄÃ¢y lÃ  pháº§n quan trá»ng nháº¥t cá»§a Multi-Component OptiCAM** - giáº£i thÃ­ch cÃ¡ch táº¡o ra K components tá»« $K_\ell = 2048$ channels.

##### BÆ°á»›c 1: Khá»Ÿi Táº¡o Learnable Weights

Vá»›i má»—i image trong batch $B$ vÃ  má»—i component $j \in \{1, 2, ..., K\}$:

$$
\mathbf{U}_{\text{raw}} \in \mathbb{R}^{B \times K \times K_\ell \times 1 \times 1}
$$

**Multi-Component há»— trá»£ 3 chiáº¿n lÆ°á»£c khá»Ÿi táº¡o** (parameter `init_method`):

**1. Adaptive initialization** (default, baseline-compatible):
- **K=1**: $\mathbf{U}_{\text{raw}} = 0.5$ (constant, giá»‘ng Baseline)
- **K>1**: $\mathbf{U}_{\text{raw}} = 0.5 + \mathcal{N}(0, 10^{-4})$ (constant + tiny noise)

$$
u_{\text{raw}, b,j,k} = \begin{cases}
0.5 & \text{if } K = 1 \\
0.5 + \epsilon_{b,j,k}, \quad \epsilon \sim \mathcal{N}(0, 10^{-4}) & \text{if } K > 1
\end{cases}
$$

**LÃ½ do:** 
- K=1: Compatible vá»›i Baseline (deterministic, reproducible)
- K>1: Tiny noise breaks symmetry giá»¯a components mÃ  khÃ´ng thay Ä‘á»•i initialization scale quÃ¡ nhiá»u
- **Symmetry breaking critical**: Náº¿u táº¥t cáº£ $\mathbf{u}_j$ giá»‘ng há»‡t nhau â†’ K components sáº½ há»c giá»‘ng nhau (vÃ´ nghÄ©a!)

**2. Random initialization**:

$$
\mathbf{U}_{\text{raw}} \sim \mathcal{N}(0, 0.01)
$$

- Random Gaussian vá»›i std=0.01
- Breaks symmetry máº¡nh, nhÆ°ng khÃ´ng baseline-compatible (K=1 cho káº¿t quáº£ khÃ¡c Baseline)

**3. Constant initialization** (âš ï¸ only safe for K=1):

$$
\mathbf{U}_{\text{raw}} = 0.5
$$

- Giá»‘ng Baseline hoÃ n toÃ n
- âŒ **WARNING**: Vá»›i K>1, táº¥t cáº£ components giá»‘ng nhau â†’ symmetry problem!

**Ã nghÄ©a chung:** 
- Má»—i component $j$ cÃ³ **má»™t bá»™ trá»ng sá»‘ riÃªng** cho táº¥t cáº£ $K_\ell = 2048$ channels
- Shape $(B, K, K_\ell, 1, 1)$ tÆ°Æ¡ng á»©ng: (batch, **components**, **channels**, spatial_h, spatial_w)
- Initialization strategy quyáº¿t Ä‘á»‹nh convergence behavior vÃ  baseline compatibility

##### BÆ°á»›c 2: Softmax Normalization TrÃªn Channel Dimension

Äá»ƒ Ä‘áº£m báº£o weights khÃ´ng explode vÃ  cÃ³ Ã½ nghÄ©a "importance", Ã¡p dá»¥ng softmax:

$$
\mathbf{W} = \text{softmax}(\mathbf{U}_{\text{raw}}, \text{dim}=\text{channel}) \in \mathbb{R}^{B \times K \times K_\ell \times 1 \times 1}
$$

Chi tiáº¿t:

$$
w_{b,j,k,1,1} = \frac{\exp(u_{\text{raw}, b,j,k})}{\sum_{k'=1}^{K_\ell} \exp(u_{\text{raw}, b,j,k'})}
$$

**Giáº£i thÃ­ch indices:**
- $b$: batch index (image nÃ o)
- $j$: **component** index (component nÃ o, $j \in \{1,2,3\}$)
- $k$: **channel** index (channel nÃ o, $k \in \{1..2048\}$)

**TÃ­nh cháº¥t quan trá»ng:**

$$
\sum_{k=1}^{K_\ell} w_{b,j,k,1,1} = 1 \quad \forall b, j
$$

**Ã nghÄ©a:**
- Má»—i component $j$ phÃ¢n phá»‘i **100% attention** lÃªn $K_\ell$ channels
- Channel $k$ nÃ o cÃ³ $w_{j,k}$ cao â†’ channel Ä‘Ã³ quan trá»ng hÆ¡n cho component $j$
- Softmax Ä‘áº£m báº£o numerical stability (khÃ´ng cÃ³ weight Ã¢m hoáº·c quÃ¡ lá»›n)

##### BÆ°á»›c 3: Linear Combination vá»›i Feature Maps

Feature maps tá»« target layer (Ä‘Ã£ qua ReLU):

$$
\mathbf{f} = \text{ReLU}(\text{Layer}_\ell(\mathbf{x})) \in \mathbb{R}^{B \times K_\ell \times h_\ell \times w_\ell}
$$

Expand Ä‘á»ƒ broadcast:

$$
\mathbf{f}_{\text{exp}} = \text{unsqueeze}(\mathbf{f}, \text{dim}=1) \in \mathbb{R}^{B \times 1 \times K_\ell \times h_\ell \times w_\ell}
$$

TÃ­nh weighted sum cho má»—i component:

$$
S^{(j)}_\ell = \sum_{k=1}^{K_\ell} w_{b,j,k} \cdot f_{b,k,:,:} \in \mathbb{R}^{h_\ell \times w_\ell}
$$

Trong code (vectorized):

$$
\mathbf{S} = (\mathbf{W} \odot \mathbf{f}_{\text{exp}}).\text{sum}(\text{dim}=\text{channel}) \in \mathbb{R}^{B \times K \times h_\ell \times w_\ell}
$$

**Ã nghÄ©a:**
- Má»—i component $S^{(j)}$ lÃ  **tá»• há»£p tuyáº¿n tÃ­nh cÃ³ trá»ng sá»‘** cá»§a Táº¤T Cáº¢ $K_\ell = 2048$ channels
- Component $j=1$ cÃ³ thá»ƒ há»c weight cao cho channels $k \in \{10, 50, 200\}$ (vÃ­ dá»¥: "Ä‘áº§u chÃ³")
- Component $j=2$ cÃ³ thá»ƒ há»c weight cao cho channels $k \in \{500, 1000, 1500\}$ (vÃ­ dá»¥: "thÃ¢n chÃ³")
- Component $j=3$ cÃ³ thá»ƒ há»c weight cao cho channels khÃ¡c (vÃ­ dá»¥: "background")

##### BÆ°á»›c 4: Upsample vÃ  Normalize vá» [0,1]

Saliency maps á»Ÿ resolution tháº¥p ($h_\ell \times w_\ell = 14 \times 14$), cáº§n upsample vá» input size $(H \times W = 224 \times 224)$:

$$
S^{(j)}_{\text{up}} = \text{Upsample}(S^{(j)}_\ell, \text{size}=(H, W), \text{mode}=\text{bilinear}) \in \mathbb{R}^{H \times W}
$$

Normalize vá» $[0, 1]$ báº±ng min-max normalization:

$$
m_j = \frac{S^{(j)}_{\text{up}} - \min(S^{(j)}_{\text{up}})}{\max(S^{(j)}_{\text{up}}) - \min(S^{(j)}_{\text{up}}) + \epsilon} \in [0,1]^{H \times W}
$$

**Káº¿t quáº£:** K masks $\{m_1, m_2, ..., m_K\}$, má»—i mask trong khoáº£ng $[0, 1]$ vÃ  cÃ³ spatial resolution $(H, W)$.

##### BÆ°á»›c 5: Backpropagation Ä‘á»ƒ Há»c Weights

**Forward pass:**

$$
m_j \to \mathbf{x}_j = m_j \odot \mathbf{x} \to p_j = \text{softmax}(f(\mathbf{x}_j))_c
$$

$$
\mathcal{L} = \mathcal{L}_{\text{fidelity}} + \lambda_t \mathcal{L}_{\text{consistency}}
$$

**Backward pass (gradient flow):**

$$
\frac{\partial \mathcal{L}}{\partial \mathbf{U}_{\text{raw}}} = \frac{\partial \mathcal{L}}{\partial p_j} \cdot \frac{\partial p_j}{\partial \mathbf{x}_j} \cdot \frac{\partial \mathbf{x}_j}{\partial m_j} \cdot \frac{\partial m_j}{\partial S^{(j)}_{\text{up}}} \cdot \frac{\partial S^{(j)}_{\text{up}}}{\partial S^{(j)}_\ell} \cdot \frac{\partial S^{(j)}_\ell}{\partial \mathbf{W}} \cdot \frac{\partial \mathbf{W}}{\partial \mathbf{U}_{\text{raw}}}
$$

**Adam optimizer update:**

$$
\mathbf{U}_{\text{raw}}^{(t+1)} = \mathbf{U}_{\text{raw}}^{(t)} - \eta \cdot \text{Adam}\left(\frac{\partial \mathcal{L}}{\partial \mathbf{U}_{\text{raw}}}\right)
$$

**Ã nghÄ©a:**
- Gradient signal tá»« loss $\mathcal{L}$ flow ngÆ°á»£c vá» weights $\mathbf{U}_{\text{raw}}$
- Weights Ä‘Æ°á»£c cáº­p nháº­t Ä‘á»ƒ:
  - **Fidelity loss tháº¥p:** Combined mask giá»¯ Ä‘Æ°á»£c confidence gá»‘c
  - **Consistency loss tháº¥p:** Tá»•ng weighted components â‰ˆ original score
- Sau cÃ¡c iterations, weights há»™i tá»¥ â†’ K components há»c Ä‘Æ°á»£c semantic parts riÃªng biá»‡t

##### TÃ³m Táº¯t: Pipeline Táº¡o K Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input: Feature maps f âˆˆ â„^(BÃ—K_â„“Ã—h_â„“Ã—w_â„“)                     â”‚
â”‚        B=batch, K_â„“=2048 channels, h_â„“Ã—w_â„“=14Ã—14              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Learnable Weights: U_raw âˆˆ â„^(BÃ—KÃ—K_â„“Ã—1Ã—1)                     â”‚
â”‚ K=3 components, K_â„“=2048 channels                              â”‚
â”‚ Initialized: U_raw ~ N(0, 0.01)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Softmax Normalization: W = softmax(U_raw, dim=channel)         â”‚
â”‚ Property: Î£_k w_{j,k} = 1  (component j, sum over channels k)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Linear Combination: S^(j) = Î£_k w_{j,k} Ã— f_k                  â”‚
â”‚ j=component index (1..K), k=channel index (1..K_â„“)             â”‚
â”‚ Output: K saliency maps âˆˆ â„^(BÃ—KÃ—h_â„“Ã—w_â„“)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Upsample + Normalize: m_j = normalize(upsample(S^(j)))         â”‚
â”‚ Output: K masks âˆˆ [0,1]^(BÃ—KÃ—HÃ—W), HÃ—W=224Ã—224                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Backpropagation: âˆ‚L/âˆ‚U_raw via Adam optimizer                  â”‚
â”‚ Iterations: 100 steps with lr=0.001 or 0.1                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

##### VÃ­ Dá»¥ Cá»¥ Thá»ƒ: K=3 Components Cho áº¢nh ChÃ³

**Ban Ä‘áº§u (iteration 0):** Weights random â†’ 3 masks giá»‘ng nhau (noisy)

**Sau training (iteration 100):** Weights learned â†’ 3 masks khÃ¡c biá»‡t:

| Component | Learned Weights (vÃ­ dá»¥) | Semantic Meaning | Visualization |
|-----------|--------------------------|------------------|---------------|
| **Component 1** | $w_{1,10}=0.05, w_{1,50}=0.08, ..., w_{1,200}=0.03$ | "Äáº§u chÃ³" (head, ears) | ğŸ• Bright á»Ÿ vÃ¹ng Ä‘áº§u |
| **Component 2** | $w_{2,500}=0.06, w_{2,1000}=0.04, ..., w_{2,1500}=0.02$ | "ThÃ¢n chÃ³" (body, legs) | ğŸ• Bright á»Ÿ thÃ¢n |
| **Component 3** | $w_{3,100}=0.02, w_{3,800}=0.01, ..., w_{3,2000}=0.03$ | "Background context" | ğŸ• Bright á»Ÿ ná»n |

**LÆ°u Ã½:** Weights cá»¥ thá»ƒ lÃ  vÃ­ dá»¥ minh há»a - thá»±c táº¿ Ä‘Æ°á»£c há»c tá»± Ä‘á»™ng qua optimization.

##### So SÃ¡nh: K=3 Components vs K_â„“=2048 Channels

| Aspect | K=3 Components (hiá»‡n táº¡i) | K_â„“=2048 Channels (thesis ideal?) |
|--------|---------------------------|-----------------------------------|
| **KhÃ¡i niá»‡m** | 3 **semantic groups** learned | 2048 **raw channels** riÃªng láº» |
| **Má»—i mask** | Linear combination of ALL 2048 channels | 1 channel duy nháº¥t |
| **Learnable params** | $3 \times 2048 = 6,144$ weights | KhÃ´ng cÃ³ (chá»‰ scaling) |
| **Optimization** | 100 iterations Adam | KhÃ´ng cáº§n (trá»±c tiáº¿p tá»« features) |
| **Forward passes** | $3 + 1 = 4$ masks (components + combined) | $2048$ masks (má»—i channel 1 mask) |
| **Computational cost** | ~14 phÃºt / 70 áº£nh | ~4 ngÃ y / 70 áº£nh |
| **Semantic level** | **High-level semantic parts** | **Low-level features** |
| **Interpretability** | âœ… Dá»… giáº£i thÃ­ch (3 parts) | âŒ KhÃ³ (2048 channels quÃ¡ nhiá»u) |

#### 3.3.3. Masked Images vÃ  Component Scores

**K masked images:**

$$
\mathbf{x}_j = m_j \odot \mathbf{x} \in \mathbb{R}^{3 \times H \times W} \quad \text{for } j=1..K
$$

**K component scores (trong probability space - KHÃC vá»›i baseline dÃ¹ng logit):**

$$
p_j = \text{softmax}(f(\mathbf{x}_j))_c \in [0,1] \quad \text{for } j=1..K
$$

**LÃ½ do dÃ¹ng probability thay vÃ¬ logit:** Xem Section 4.1 vá» Pure Probability Space Formulation.

#### 3.3.4. Combined Mask vÃ  Reconstruction

**Weighted combination of masks:**

$$
m_{\text{combined}} = \text{clamp}\left(\sum_{j=1}^{K} \beta_j \cdot m_j, 0, 1\right)
$$

**Giáº£i thÃ­ch tá»«ng thÃ nh pháº§n:**
- $m_j \in [0,1]^{H \times W}$: Mask cá»§a component $j$ (má»—i pixel trong khoáº£ng $[0,1]$)
- $\beta_j \in (0,1)$: Importance weight cá»§a component $j$, vá»›i $\sum_{j=1}^K \beta_j = 1$
- $\beta_j \cdot m_j$: Scale mask $j$ theo importance - element-wise multiplication
- $\sum_{j=1}^{K} \beta_j \cdot m_j$: Tá»•ng cÃ³ trá»ng sá»‘ cá»§a K masks - weighted average
- **Váº¥n Ä‘á»:** Tá»•ng cÃ³ thá»ƒ > 1 (e.g., náº¿u nhiá»u masks overlap á»Ÿ cÃ¹ng vÃ¹ng)
- $\text{clamp}(\cdot, 0, 1)$: Clip giÃ¡ trá»‹ vá» khoáº£ng $[0,1]$ - $\min(\max(\text{value}, 0), 1)$
- **Káº¿t quáº£:** $m_{\text{combined}} \in [0,1]^{H \times W}$ - valid mask

**LÆ°u Ã½:** Clamp vá» $[0,1]$ Ä‘á»ƒ Ä‘áº£m báº£o valid mask (vÃ¬ tá»•ng cÃ³ trá»ng sá»‘ cÃ³ thá»ƒ vÆ°á»£t 1).

**Táº¡i sao cáº§n clamp?**
- Náº¿u 3 masks Ä‘á»u = 1 á»Ÿ cÃ¹ng pixel vÃ  $\boldsymbol{\beta} = [0.4, 0.3, 0.3]$ â†’ sum = 1.0 (OK)
- NhÆ°ng náº¿u masks overlap khÃ¡c nhau â†’ cÃ³ pixel sum > 1 â†’ cáº§n clamp
- Clamp Ä‘áº£m báº£o mask luÃ´n valid cho element-wise multiplication vá»›i image

**Combined masked image:**

$$
\mathbf{x}_{\text{combined}} = m_{\text{combined}} \odot \mathbf{x}
$$

**Giáº£i thÃ­ch:**
- $m_{\text{combined}} \in [0,1]^{H \times W}$: Combined mask (2D)
- $\mathbf{x} \in \mathbb{R}^{3 \times H \times W}$: Original RGB image (3D)
- $\odot$: Element-wise multiplication vá»›i broadcasting (mask 2D â†’ 3D)
- $\mathbf{x}_{\text{combined}}$: Masked image - giá»‘ng baseline nhÆ°ng mask lÃ  weighted combination

**Combined score (probability space):**

$$
p_{\text{combined}} = \text{softmax}(f(\mathbf{x}_{\text{combined}}))_c
$$

**Giáº£i thÃ­ch:**
- $f(\mathbf{x}_{\text{combined}})$: Forward pass qua CNN, output logits $\in \mathbb{R}^C$
- $\text{softmax}(\cdot)_c$: Convert logits â†’ probabilities, láº¥y class $c$
- $p_{\text{combined}} \in [0,1]$: Probability cá»§a class $c$ cho combined masked image
- **Má»¥c Ä‘Ã­ch:** So sÃ¡nh vá»›i $p_{\text{orig}}$ trong fidelity loss

**Original score (probability space):**

$$
p_{\text{orig}} = \text{softmax}(f(\mathbf{x}))_c
$$

**Giáº£i thÃ­ch:**
- $\mathbf{x}$: Original image (khÃ´ng mask)
- $f(\mathbf{x})$: Logits tá»« original image
- $p_{\text{orig}} \in [0,1]$: Ground truth probability - baseline Ä‘á»ƒ so sÃ¡nh
- **Vai trÃ²:** Reference value trong cáº£ fidelity loss vÃ  consistency loss

---

## 4. HÃ m Má»¥c TiÃªu vÃ  Loss Functions

### 4.1. Pure Probability Space Formulation

**Quan sÃ¡t quan trá»ng:** OptiCAM baseline maximize **logit** $y_c$ (Equation 10), nhÆ°ng Multi-Component cáº§n **constraint** giá»¯a cÃ¡c components â†’ cáº§n scale phÃ¹ há»£p.

**Logit space:** $y_c \in (-\infty, +\infty)$ - khÃ´ng bounded, khÃ³ so sÃ¡nh vÃ  cá»™ng
**Probability space:** $p_c \in [0, 1]$ - bounded, mathematically valid cho additivity

**LÃ½ do chá»n Probability Space cho Multi-Component:**

1. **Consistency constraint cÃ³ nghÄ©a:** $\sum_{j=1}^K \beta_j p_j \approx p_{\text{orig}}$ - cáº£ 2 váº¿ Ä‘á»u trong $[0,1]$
2. **Same scale cho fidelity vÃ  consistency:** KhÃ´ng cáº§n tune $\lambda$ phá»©c táº¡p
3. **Interpretable violation:** $|v| = 0.1$ nghÄ©a lÃ  sai lá»‡ch 10% probability (rÃµ rÃ ng)

**Trade-off:** Lose má»™t chÃºt "directness" cá»§a logit space (nhÆ° OptiCAM baseline), nhÆ°ng gain mathematical correctness vÃ  stability.

### 4.2. Multi-Component Loss Function

**LÆ°u Ã½ vá» terminology:**
- OptiCAM baseline: **Maximize objective** $F^c_\ell$ (Equation 9-10) - khÃ´ng cÃ³ explicit loss
- Multi-Component: **Minimize loss** $\mathcal{L}$ vá»›i 2 components - do cÃ³ constraint

#### 4.2.1. Fidelity Loss

**Má»¥c tiÃªu:** Combined mask pháº£i báº£o toÃ n confidence gá»‘c (tÆ°Æ¡ng tá»± OptiCAM baseline objective).

$$
\mathcal{L}_{\text{fidelity}} = \frac{1}{B} \sum_{i=1}^{B} \left( p_{\text{orig},i} - p_{\text{combined},i} \right)^2
$$

**Giáº£i thÃ­ch tá»«ng thÃ nh pháº§n:**
- $B$: Batch size - sá»‘ images xá»­ lÃ½ cÃ¹ng lÃºc (e.g., $B = 10$)
- $i$: Index cá»§a image trong batch, $i \in \{1, 2, ..., B\}$
- $p_{\text{orig},i}$: Original probability cho image thá»© $i$ - ground truth value
- $p_{\text{combined},i}$: Combined masked image probability cho image thá»© $i$
- $(p_{\text{orig},i} - p_{\text{combined},i})^2$: Squared error cho image $i$ - MSE loss per sample
- $\sum_{i=1}^{B}$: Tá»•ng squared errors cá»§a táº¥t cáº£ images trong batch
- $\frac{1}{B}$: Trung bÃ¬nh (average) - normalize theo batch size
- **Káº¿t quáº£:** Scalar value $\in [0, 1]$ (vÃ¬ probabilities $\in [0,1]$, squared error â‰¤ 1)

Vá»›i:
- $B$: Batch size
- $p_{\text{orig}} = \text{softmax}(f(\mathbf{x}))_c$ - original confidence
- $p_{\text{combined}} = \text{softmax}(f(\mathbf{x}_{\text{combined}}))_c$ - combined confidence

**Ã nghÄ©a:** 
- TÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i **maximizing** $p_{\text{combined}}$ Ä‘á»ƒ gáº§n $p_{\text{orig}}$
- Äáº£m báº£o aggregated mask váº«n giá»¯ Ä‘Æ°á»£c kháº£ nÄƒng dá»± Ä‘oÃ¡n cá»§a áº£nh gá»‘c
- **Analog cá»§a OptiCAM baseline objective** (Equation 10) nhÆ°ng á»Ÿ probability space
- Minimize MSE = maximize similarity giá»¯a combined vÃ  original scores

#### 4.2.2. Consistency Loss (Decomposition Constraint)

**Má»¥c tiÃªu:** Tá»•ng cÃ¡c component scores â‰ˆ original score - **Ä‘Ã¢y lÃ  pháº§n Má»šI**, khÃ´ng cÃ³ trong OptiCAM baseline.

$$
\mathcal{L}_{\text{consistency}} = \frac{1}{B} \sum_{i=1}^{B} \left( p_{\text{orig},i} - \sum_{j=1}^{K} \beta_j \cdot p_{j,i} \right)^2
$$

**Giáº£i thÃ­ch tá»«ng thÃ nh pháº§n:**
- $K$: Number of components (e.g., $K = 3$)
- $\beta_j$: Importance weight cá»§a component $j$, vá»›i $\sum_{j=1}^K \beta_j = 1$
- $p_{j,i}$: Probability cá»§a component $j$ cho image $i$ - tá»« $\mathbf{x}_j = m_j \odot \mathbf{x}_i$
- $\beta_j \cdot p_{j,i}$: Weighted contribution cá»§a component $j$ cho image $i$
- $\sum_{j=1}^{K} \beta_j \cdot p_{j,i}$: Tá»•ng weighted contributions cá»§a táº¥t cáº£ K components
- $p_{\text{orig},i} - \sum_{j=1}^{K} \beta_j \cdot p_{j,i}$: Constraint violation - sai lá»‡ch giá»¯a tá»•ng vÃ  original
- $(\cdot)^2$: Squared Ä‘á»ƒ cÃ³ non-negative loss vÃ  penalize large violations
- $\frac{1}{B} \sum_{i=1}^{B}$: Average over batch

**Chi tiáº¿t:**

$$
\text{Sum of component scores: } \quad p_{\text{sum}} = \sum_{j=1}^{K} \beta_j \cdot p_j
$$

**Giáº£i thÃ­ch:**
- $p_{\text{sum}}$: Prediction "reconstructed" tá»« K components
- **Ã nghÄ©a toÃ¡n há»c:** Náº¿u components decompose correctly, tá»•ng weighted scores = original score
- **VÃ­ dá»¥:** $p_1=0.4$, $p_2=0.3$, $p_3=0.15$, $\boldsymbol{\beta}=[0.33, 0.33, 0.34]$ â†’ $p_{\text{sum}} = 0.4(0.33) + 0.3(0.33) + 0.15(0.34) \approx 0.28$

$$
\text{Constraint violation: } \quad v = p_{\text{sum}} - p_{\text{orig}}
$$

**Giáº£i thÃ­ch:**
- $v$: Violation of decomposition constraint - sai sá»‘
- $v > 0$: Components overestimate (tá»•ng > original) â†’ cáº§n giáº£m component scores
- $v < 0$: Components underestimate (tá»•ng < original) â†’ cáº§n tÄƒng component scores  
- $v = 0$: Perfect decomposition (ideal case)
- **Interpretable:** $|v| = 0.1$ nghÄ©a lÃ  sai lá»‡ch 10% probability

$$
\mathcal{L}_{\text{consistency}} = \mathbb{E}[v^2] = \frac{1}{B} \sum_{i=1}^{B} v_i^2
$$

**Giáº£i thÃ­ch:**
- $\mathbb{E}[\cdot]$: Expectation operator - trung bÃ¬nh theo batch
- $v^2$: Squared violation - non-negative, penalize cáº£ positive vÃ  negative violations
- **Ã nghÄ©a:** Mean squared error cá»§a constraint violation

**Ã nghÄ©a mathematically:**
- Khi $\mathcal{L}_{\text{consistency}} \to 0$: $\sum_{j=1}^K \beta_j \cdot p_j \approx p_{\text{orig}}$
- NghÄ©a lÃ  cÃ¡c components "cá»™ng láº¡i" Ä‘Ãºng báº±ng áº£nh hÆ°á»Ÿng gá»‘c
- ÄÃ¢y lÃ  **soft constraint** (khÃ´ng enforce hard = 0, cho phÃ©p small violation)

**LÆ°u Ã½ quan trá»ng:** ÄÃ¢y lÃ  Ä‘iá»ƒm khÃ¡c biá»‡t chÃ­nh vá»›i OptiCAM baseline - baseline chá»‰ cÃ³ 1 mask nÃªn khÃ´ng cáº§n constraint nÃ y.

#### 4.2.3. Táº¡i Sao DÃ¹ng Probability Space?

**Váº¥n Ä‘á» vá»›i Logit Space:**

Náº¿u dÃ¹ng logits: $\sum_{j=1}^{K} \beta_j \cdot y_{\text{logit},j} \approx y_{\text{logit},orig}$

- âŒ Logits khÃ´ng bounded: $y_{\text{logit}} \in (-\infty, +\infty)$
- âŒ KhÃ´ng cÃ³ tÃ­nh cháº¥t cá»™ng tÃ­nh (additivity) - khÃ´ng Ä‘áº£m báº£o tá»•ng cÃ³ nghÄ©a
- âŒ Scale khÃ¡c nhau giá»¯a cÃ¡c classes (má»™t sá»‘ class cÃ³ logit ráº¥t cao/tháº¥p)

**Æ¯u Ä‘iá»ƒm Probability Space:**

âœ… **Bounded:** $p \in [0,1]$ - dá»… kiá»ƒm soÃ¡t vÃ  diá»…n giáº£i
âœ… **Additivity valid:** Probabilities cÃ³ thá»ƒ cá»™ng (nhÆ° phÃ¢n phá»‘i rá»i ráº¡c)
âœ… **Same scale:** Táº¥t cáº£ components cÃ¹ng scale [0,1] â†’ Î» cÃ³ Ã½ nghÄ©a
âœ… **Interpretable:** Constraint violation `v = 0.1` nghÄ©a lÃ  sai lá»‡ch 10% probability

#### 4.2.4. Total Loss vá»›i Lambda Scheduling

**Weighted combination:**

$$
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{fidelity}} + \lambda_t \cdot \mathcal{L}_{\text{consistency}}
$$

#### 4.2.4. Total Loss vá»›i Lambda Scheduling

**Weighted combination:**

$$
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{fidelity}} + \lambda_t \cdot \mathcal{L}_{\text{consistency}}
$$

**Giáº£i thÃ­ch tá»«ng thÃ nh pháº§n:**
- $\mathcal{L}_{\text{fidelity}}$: Fidelity loss (scalar) - Ä‘áº£m báº£o combined mask faithful
- $\mathcal{L}_{\text{consistency}}$: Consistency loss (scalar) - Ä‘áº£m báº£o decomposition correct
- $\lambda_t$: Weighting parameter (scalar) - balance giá»¯a 2 objectives, phá»¥ thuá»™c iteration $t$
- $\lambda_t \cdot \mathcal{L}_{\text{consistency}}$: Weighted consistency - control importance
- $+$: Cá»™ng 2 losses - multi-objective optimization
- $\mathcal{L}_{\text{total}}$: Single scalar loss Ä‘á»ƒ minimize báº±ng gradient descent

**Táº¡i sao cáº§n lambda?**
- 2 losses cÃ³ **objectives khÃ¡c nhau**: fidelity (faithfulness) vs consistency (decomposition)
- KhÃ´ng cÃ³ $\lambda$ â†’ 2 losses equally important â†’ cÃ³ thá»ƒ conflict
- $\lambda$ cho phÃ©p **trade-off**: $\lambda$ cao = Æ°u tiÃªn consistency, $\lambda$ tháº¥p = Æ°u tiÃªn fidelity

**Adaptive lambda scheduling:**

$$
\lambda_t = \lambda_{\text{start}} - \left(\lambda_{\text{start}} - \lambda_{\text{end}}\right) \cdot \frac{t}{T-1}
$$

**Giáº£i thÃ­ch tá»«ng thÃ nh pháº§n:**
- $t$: Current iteration number, $t \in \{0, 1, 2, ..., T-1\}$
- $T$: Total iterations (max_iter), e.g., $T = 100$
- $\lambda_{\text{start}}$: Initial lambda value, e.g., $\lambda_{\text{start}} = 1.0$ (high)
- $\lambda_{\text{end}}$: Final lambda value, e.g., $\lambda_{\text{end}} = 0.3$ (lower)
- $\lambda_{\text{start}} - \lambda_{\text{end}}$: Total decay amount, e.g., $1.0 - 0.3 = 0.7$
- $\frac{t}{T-1}$: Progress ratio $\in [0, 1]$ - at $t=0$ â†’ 0, at $t=T-1$ â†’ 1
- $(\lambda_{\text{start}} - \lambda_{\text{end}}) \cdot \frac{t}{T-1}$: Decay amount at iteration $t$
- $\lambda_t = \lambda_{\text{start}} - \text{decay}$: Linear interpolation tá»« start â†’ end

**VÃ­ dá»¥ tÃ­nh toÃ¡n:**
- Iteration $t=0$: $\lambda_0 = 1.0 - (1.0-0.3) \cdot \frac{0}{99} = 1.0$ (start)
- Iteration $t=50$: $\lambda_{50} = 1.0 - 0.7 \cdot \frac{50}{99} \approx 0.65$ (mid)
- Iteration $t=99$: $\lambda_{99} = 1.0 - 0.7 \cdot \frac{99}{99} = 0.3$ (end)

Vá»›i:
- $t$ = current iteration (0 to T-1)
- $T$ = `max_iter` (e.g., 100)
- $\lambda_{\text{start}} = 1.0$ (high consistency pressure initially)
- $\lambda_{\text{end}} = 0.3$ (reduce to focus on fidelity)

**Intuition:**
- **Early iterations (Î» high):** Enforce consistency â†’ components learn to decompose correctly
  - $t=0$: $\lambda = 1.0$ â†’ $\mathcal{L} = \mathcal{L}_{\text{fid}} + 1.0 \times \mathcal{L}_{\text{cons}}$ (equal weight)
  - Components bá»‹ "force" pháº£i satisfy constraint $\sum_{j=1}^{K} \beta_j p_j \approx p_{\text{orig}}$
  
- **Late iterations (Î» low):** Focus on fidelity â†’ fine-tune combined mask quality  
  - $t=99$: $\lambda = 0.3$ â†’ $\mathcal{L} = \mathcal{L}_{\text{fid}} + 0.3 \times \mathcal{L}_{\text{cons}}$ (fidelity dominant)
  - Optimizer Æ°u tiÃªn maximize combined mask faithfulness, consistency lÃ  soft constraint

**Táº¡i sao schedule (khÃ´ng pháº£i constant)?**
- **Constant $\lambda$:** Hard to tune - quÃ¡ cao â†’ poor fidelity, quÃ¡ tháº¥p â†’ poor consistency
- **Scheduling:** Best of both - start vá»›i strong constraint, end vá»›i focus on quality
- **Adaptive:** Components há»c structure Ä‘Ãºng early, refine quality later

**Visualization:**

```
Î»
â”‚ Î»_start=1.0  â—
â”‚               â•²
â”‚                â•²    Linear decay
â”‚                 â•²
â”‚                  â•²
â”‚ Î»_end=0.3         â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ t
  0              50            100
```

**CÃ´ng thá»©c tá»•ng quÃ¡t (linear interpolation):**

Cho 2 Ä‘iá»ƒm $(t_0, y_0)$ vÃ  $(t_1, y_1)$, giÃ¡ trá»‹ táº¡i $t$ lÃ :

$$
y_t = y_0 + (y_1 - y_0) \cdot \frac{t - t_0}{t_1 - t_0}
$$

Vá»›i $t_0=0$, $t_1=T-1$, $y_0=\lambda_{\text{start}}$, $y_1=\lambda_{\text{end}}$:

$$
\lambda_t = \lambda_{\text{start}} + (\lambda_{\text{end}} - \lambda_{\text{start}}) \cdot \frac{t}{T-1}
$$

Viáº¿t láº¡i: $\lambda_t = \lambda_{\text{start}} - (\lambda_{\text{start}} - \lambda_{\text{end}}) \cdot \frac{t}{T-1}$ (tÆ°Æ¡ng Ä‘Æ°Æ¡ng)

---

## 5. Tá»‘i Æ¯u HÃ³a: Adam Optimizer vÃ  Mixed Precision

### 5.1. Táº¡i Sao Adam?

**So sÃ¡nh vá»›i cÃ¡c optimizers:**

| Optimizer | Update Rule | Æ¯u Äiá»ƒm | NhÆ°á»£c Äiá»ƒm |
|-----------|-------------|---------|------------|
| **SGD** | $\theta \gets \theta - \eta \nabla L$ | ÄÆ¡n giáº£n, á»•n Ä‘á»‹nh | Cháº­m, cáº§n tune LR carefully |
| **SGD+Momentum** | Cá»™ng thÃªm momentum | Nhanh hÆ¡n SGD | Váº«n cáº§n tune |
| **RMSprop** | Adaptive LR per-parameter | Tá»± Ä‘á»™ng scale | KhÃ´ng cÃ³ bias correction |
| **Adam** | Momentum + RMSprop + Bias correction | Robust, Ã­t tune, nhanh | Memory overhead (lÆ°u m, v) |

**Adam (Adaptive Moment Estimation):**

$$
m_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla_{\theta} \mathcal{L}_t
$$

**Giáº£i thÃ­ch (First moment - momentum):**
- $\theta$: Parameters (weights) cáº§n optimize, e.g., $\mathbf{U}, \boldsymbol{\beta}$
- $\nabla_{\theta} \mathcal{L}_t$: Gradient cá»§a loss theo $\theta$ táº¡i iteration $t$ (vector cÃ¹ng shape vá»›i $\theta$)
- $m_{t-1}$: Momentum tá»« iteration trÆ°á»›c (exponential moving average cá»§a gradients)
- $\beta_1$: Decay rate cho momentum, thÆ°á»ng $\beta_1 = 0.9$ (keep 90% history)
- $(1 - \beta_1)$: Weight cho current gradient, $1 - 0.9 = 0.1$ (10% new info)
- $m_t$: Updated momentum - weighted average cá»§a past vÃ  current gradients
- **Ã nghÄ©a:** Smooth gradient fluctuations, accelerate in consistent directions

$$
v_t = \beta_2 v_{t-1} + (1 - \beta_2) (\nabla_{\theta} \mathcal{L}_t)^2
$$

$$
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
$$

$$
v_t = \beta_2 v_{t-1} + (1 - \beta_2) (\nabla_{\theta} \mathcal{L}_t)^2
$$

**Giáº£i thÃ­ch (Second moment - RMSprop):**
- $(\nabla_{\theta} \mathcal{L}_t)^2$: Element-wise square cá»§a gradient (not matrix multiplication!) - measure gradient magnitude
- $v_{t-1}$: Variance estimate tá»« iteration trÆ°á»›c (exponential moving average cá»§a squared gradients)
- $\beta_2$: Decay rate cho variance, thÆ°á»ng $\beta_2 = 0.999$ (keep 99.9% history)
- $(1 - \beta_2)$: Weight cho current squared gradient, $1 - 0.999 = 0.001$ (0.1% new info)
- $v_t$: Updated variance - tracks "how much gradients vary"
- **Ã nghÄ©a:** Estimate variance of gradients, used to scale learning rate per parameter

$$
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
$$

**Giáº£i thÃ­ch (Bias correction):**
- $m_t, v_t$: Raw moments (biased toward 0 initially because $m_0 = 0, v_0 = 0$)
- $\beta_1^t$: $\beta_1$ raised to power $t$ - exponential decay ($0.9^{10} \approx 0.35$)
- $1 - \beta_1^t$: Bias correction factor - at $t=1$ â†’ $1-0.9=0.1$, at $t=10$ â†’ $1-0.35=0.65$, at $t \to \infty$ â†’ 1
- $\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$: Corrected momentum - unbiased estimate
- $\beta_2^t$: Similar for variance ($0.999^{100} \approx 0.905$)
- $\hat{v}_t$: Corrected variance
- **Táº¡i sao cáº§n?** Early iterations: $m_t, v_t$ small â†’ correction scales them up â†’ faster initial learning

$$
\theta_{t+1} = \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
$$

**Giáº£i thÃ­ch (Parameter update):**
- $\theta_t$: Current parameters (weights)
- $\eta$: Learning rate (step size), e.g., $\eta = 0.001$
- $\hat{m}_t$: Bias-corrected momentum (direction to move)
- $\sqrt{\hat{v}_t}$: Square root of variance (element-wise) - scale factor based on gradient variability
- $\epsilon$: Small constant, e.g., $\epsilon = 10^{-8}$ - prevents division by zero khi $\hat{v}_t \approx 0$
- $\frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$: **Adaptive gradient** - momentum scaled by inverse of gradient std dev
  - Parameter cÃ³ gradient stable (low variance) â†’ large step
  - Parameter cÃ³ gradient noisy (high variance) â†’ small step
- $\eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$: Final update step (element-wise operation)
- $\theta_{t+1} = \theta_t - \text{step}$: Move in negative gradient direction (gradient descent)

**VÃ­ dá»¥ cá»¥ thá»ƒ:**
```
Giáº£ sá»­: Î¸ = [wâ‚, wâ‚‚], Î· = 0.01
Iteration t=10:
  âˆ‡L = [5.0, 0.1]  (wâ‚ cÃ³ gradient lá»›n, wâ‚‚ nhá»)
  mÌ‚â‚œ = [4.5, 0.09]  (momentum smoothed)
  vÌ‚â‚œ = [20.0, 0.01]  (wâ‚ vary nhiá»u, wâ‚‚ stable)
  
  Adaptive step:
  wâ‚: -0.01 Ã— 4.5/âˆš20.0 â‰ˆ -0.01  (scaled down vÃ¬ variance cao)
  wâ‚‚: -0.01 Ã— 0.09/âˆš0.01 â‰ˆ -0.009 (scaled up vÃ¬ variance tháº¥p)
  
  Result: Parameters cÃ³ gradient noisy Ä‘Æ°á»£c move cáº©n tháº­n hÆ¡n!
```

**Hyperparameters:**
- $\beta_1 = 0.9$ (momentum decay) - keep 90% momentum history
- $\beta_2 = 0.999$ (RMSprop decay) - keep 99.9% variance history  
- $\epsilon = 10^{-8}$ (numerical stability) - tiny value to prevent division by zero

**Táº¡i sao phÃ¹ há»£p vá»›i OptiCAM:**
1. **Adaptive learning rates:** Má»—i parameter tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh LR - khÃ´ng cáº§n manual tuning per parameter
2. **Handles sparse gradients:** Tá»‘t cho optimization vá»›i mask (nhiá»u vÃ¹ng gradient = 0) - RMSprop component helps
3. **Fast convergence:** Há»™i tá»¥ nhanh (~100 iterations Ä‘á»§) - momentum accelerates
4. **Robust:** Ãt nháº¡y cáº£m vá»›i initialization - bias correction handles early iterations

### 5.2. Learning Rate Selection

**Current settings:**
- OptiCAM Baseline: `lr = 0.01`
- Multi-Component: `lr = 0.001` (1e-3)

**Táº¡i sao Multi cáº§n LR tháº¥p hÆ¡n?**
- Nhiá»u parameters hÆ¡n: $W \in \mathbb{R}^{K \times C}$ vá»›i $K=3, C=2048$ â†’ 6,144 params
- Consistency constraint nháº¡y cáº£m: pháº£i balance K components
- LR cao â†’ oscillation, khÃ³ converge Ä‘á»“ng thá»i K masks

**Suggested tuning:**
- `lr = 5e-4`: Náº¿u tháº¥y violation cao (>15%)
- `lr = 2e-3`: Náº¿u convergence quÃ¡ cháº­m

### 5.3. Mixed Precision Training

**Float16 vs Float32:**

| Aspect | FP32 | FP16 (Mixed Precision) |
|--------|------|----------------------|
| **Memory** | 4 bytes | 2 bytes (50% tiáº¿t kiá»‡m) |
| **Speed** | Baseline | ~2x nhanh (Tensor Cores) |
| **Precision** | 7 significant digits | 3 significant digits |
| **Gradient underflow** | KhÃ´ng xáº£y ra | CÃ³ thá»ƒ xáº£y ra |

**Implementation:**

```python
from torch.amp import autocast, GradScaler

scaler = GradScaler('cuda')

for step in range(max_iter):
    with autocast('cuda'):  # Forward pass in FP16
        loss = compute_loss(...)
    
    scaler.scale(loss).backward()  # Scale loss to prevent underflow
    scaler.step(optimizer)
    scaler.update()
```

**Loss Scaling:** NhÃ¢n gradient vá»›i scale factor (e.g., 2^16) Ä‘á»ƒ trÃ¡nh underflow trong FP16.

**Khi nÃ o dÃ¹ng Mixed Precision:**
- GPU há»— trá»£ Tensor Cores (RTX 20xx+, V100, A100)
- Batch size lá»›n (memory bottleneck)
- Muá»‘n tÄƒng tá»‘c 1.5-2x

---

## 6. Metrics ÄÃ¡nh GiÃ¡

### 6.1. Primary Metrics - Faithfulness

#### 6.1.1. Average Drop (AD) - Equation 13

**Äá»‹nh nghÄ©a:** Trung bÃ¬nh % confidence giáº£m khi mask **outside** salient region (keep salient, remove background) trÃªn **Táº¤T Cáº¢ N samples**. (Äá»™ giáº£m confidence sau khi chá»‰ mask má»—i hÃ¬nh áº£nh.)

$$
\text{AD} = \frac{1}{N} \sum_{i=1}^{N} \frac{|p_i^c - o_i^c|_+}{p_i^c} \times 100\%
$$

**KÃ½ hiá»‡u:**
- $p_i^c$: Original confidence (áº£nh gá»‘c)
- $o_i^c$: Masked confidence (mask **outside** salient â†’ giá»¯ salient, bá» background)
- $|x|_+ = \max(0, x)$: Positive part (chá»‰ láº¥y pháº§n giáº£m, bá» qua pháº§n tÄƒng)
- $N$: Tá»•ng sá»‘ samples (khÃ´ng pháº£i chá»‰ drop samples!)

**Masking direction:**
- **Mask outside salient region** = Keep salient pixels, remove background
- Äo xem khi chá»‰ giá»¯ láº¡i vÃ¹ng salient, confidence giáº£m bao nhiÃªu

**Ã nghÄ©a:**
- **AD tháº¥p** (e.g., 2%) â†’ Mask ráº¥t faithful, vÃ¹ng salient báº£o tá»“n háº§u háº¿t thÃ´ng tin
- **AD cao** (e.g., 20%) â†’ Mask thiáº¿u nhiá»u vÃ¹ng quan trá»ng, chá»‰ giá»¯ salient khÃ´ng Ä‘á»§
- **Zero for increase samples:** Náº¿u $o_i^c > p_i^c$ (tÄƒng), contribution = 0 (do $|x|_+$)

**Má»¥c tiÃªu:** Minimize AD.

#### 6.1.2. Average Increase (AI) - Equation 14

**Äá»‹nh nghÄ©a:** % samples cÃ³ confidence **tÄƒng** sau khi mask (unexpected behavior).

$$
\text{AI} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{1}_{p_i^c < o_i^c} \times 100\%
$$

**KÃ½ hiá»‡u:**
- $\mathbb{1}_{p_i^c < o_i^c}$: Indicator function = 1 náº¿u confidence tÄƒng, 0 náº¿u giáº£m
- TÃ­nh trÃªn **Táº¤T Cáº¢ N samples** (giá»‘ng AD vÃ  AG)

**Ã nghÄ©a:**
- **AI = 0%:** Ideal - mask chá»‰ loáº¡i bá» info, khÃ´ng thÃªm info
- **AI > 0%:** Mask loáº¡i bá» distractor noise â†’ confidence tÄƒng (cÃ³ thá»ƒ lÃ  tá»‘t)

**TrÆ°á»ng há»£p AI cao lÃ  tá»‘t:**
- Background clutter gÃ¢y nhiá»…u â†’ mask lÃ m sáº¡ch â†’ confidence tÄƒng
- Model overfitting vÃ o texture noise â†’ mask loáº¡i bá» â†’ tÄƒng

#### 6.1.3. Average Gain (AG) - Equation 15

**Äá»‹nh nghÄ©a:** Trung bÃ¬nh % confidence **DROP** khi mask **inside** salient region (remove salient, keep background) trÃªn **Táº¤T Cáº¢ N samples**. (Äá»™ tÄƒng confidence sau khi mask áº£nh.)

$$
\text{AG} = \frac{1}{N} \sum_{i=1}^{N} \frac{|o_i^c - p_i^c|_+}{1 - p_i^c} \times 100\%
$$

**KÃ½ hiá»‡u:**
- $p_i^c$: Original confidence (áº£nh gá»‘c)
- $o_i^c$: Masked confidence (mask **inside** salient â†’ bá» salient, giá»¯ background)
- $|x|_+ = \max(0, x)$: Positive part (chá»‰ láº¥y pháº§n giáº£m khi remove salient)
- Normalization: $1 - p_i^c$ = Remaining headroom (potential for increase)
- $N$: Tá»•ng sá»‘ samples (khÃ´ng pháº£i chá»‰ increase samples!)

**Masking direction:**
- **Mask inside salient region** = Remove salient pixels, keep background
- Äo xem khi **Bá»** vÃ¹ng salient, confidence giáº£m bao nhiÃªu
- **SYMMETRIC vá»›i AD:** AD mask outside (keep salient), AG mask inside (remove salient)

**âš ï¸ NOTE QUAN TRá»ŒNG:**
- **TÃªn gá»i "Gain" lÃ  MISLEADING!** CÃ´ng thá»©c Ä‘o **DROP** (giáº£m), khÃ´ng pháº£i gain (tÄƒng)
- Paper Ä‘áº·t tÃªn AG vÃ¬ normalize bá»Ÿi $(1-p)$ (potential gain), nhÆ°ng Ä‘o **|o-p|_+** = drop
- ÄÃºng hÆ¡n nÃªn gá»i "Average Drop when Mask Inside" nhÆ°ng giá»¯ tÃªn AG theo paper

**Symmetry giá»¯a AD vÃ  AG:**

| Metric | Masking | Measures | Normalize by | Positive part |
|--------|---------|----------|--------------|---------------|
| **AD** | Outside (keep salient) | Drop from original | $p$ (starting point) | $|p - o|_+$ |
| **AG** | Inside (remove salient) | Drop from original | $1-p$ (headroom) | $|o - p|_+$ |

**CÃ¹ng difference:** $o - p$, nhÆ°ng láº¥y **opposite parts** vÃ  **different normalizers**.

**Ã nghÄ©a:**
- **AG tháº¥p** â†’ Khi bá» salient, confidence giáº£m Ã­t (salient khÃ´ng quan trá»ng láº¯m)
- **AG cao** â†’ Khi bá» salient, confidence giáº£m máº¡nh (salient ráº¥t quan trá»ng)
- **Zero for non-drop samples:** Náº¿u $o_i^c \geq p_i^c$ (khÃ´ng giáº£m), contribution = 0

**Má»¥c tiÃªu:** Maximize AG (salient region cÃ ng critical cÃ ng tá»‘t).

### 6.2. Advanced Metrics - Insertion/Deletion

#### 6.2.1. Insertion AUC

**Ã tÆ°á»Ÿng:** Dáº§n dáº§n **thÃªm vÃ o** cÃ¡c patches theo thá»© tá»± importance â†’ Ä‘o confidence curve.

**Algorithm:**
1. Start: Baseline image (black hoáº·c blur) â†’ score â‰ˆ 0
2. Add patches theo thá»© tá»± decreasing saliency (important first)
3. Record scores: $s_0, s_1, ..., s_N$
4. Compute AUC: $\text{InsAUC} = \int_0^1 s(r) \, dr$ vá»›i $r$ = fraction revealed

**CÃ´ng thá»©c:**

$$
\text{InsAUC} = \frac{1}{N} \sum_{k=1}^{N} s_k
$$

(Trapezoidal integration)

**Ã nghÄ©a:**
- **InsAUC cao** (gáº§n 1.0) â†’ Mask identify Ä‘Æ°á»£c important regions early
- **InsAUC tháº¥p** â†’ Mask khÃ´ng Ä‘Ãºng priority

**Má»¥c tiÃªu:** Maximize InsAUC.

#### 6.2.2. Deletion AUC

**Ã tÆ°á»Ÿng:** Dáº§n dáº§n **xÃ³a Ä‘i** cÃ¡c patches theo thá»© tá»± importance â†’ confidence giáº£m nhanh.

**Algorithm:**
1. Start: Original image â†’ score = $p_{\text{orig}}$
2. Remove patches theo thá»© tá»± decreasing saliency (important first)
3. Record scores: $s_0, s_1, ..., s_N$ (giáº£m dáº§n)
4. Compute AUC: $\text{DelAUC} = \int_0^1 s(r) \, dr$

**Ã nghÄ©a:**
- **DelAUC tháº¥p** â†’ Mask identify important regions (removing causes sharp drop)
- **DelAUC cao** â†’ Mask khÃ´ng tá»‘t (removing khÃ´ng áº£nh hÆ°á»Ÿng)

**Má»¥c tiÃªu:** Minimize DelAUC.

#### 6.2.3. AOPC (Average Over Perturbation Curve)

**Insertion AOPC:**

$$
\text{AOPC}_{\text{ins}} = \frac{1}{N} \sum_{k=1}^{N} (s_k - s_0)
$$

Vá»›i $s_0$ = baseline score (blur/black image).

**Deletion AOPC:**

$$
\text{AOPC}_{\text{del}} = \frac{1}{N} \sum_{k=1}^{N} (s_0 - s_k)
$$

Vá»›i $s_0$ = original score.

**Ã nghÄ©a:** Trung bÃ¬nh Ä‘á»™ thay Ä‘á»•i confidence khi perturb. (Minimize Deletion AOPC, Maximize Insertion AOPC).

### 6.3. Multi-Component Specific Metrics

#### 6.3.1. Consistency Error - Tá»•ng Vi Pháº¡m RÃ ng Buá»™c

**Äá»‹nh nghÄ©a:** Tá»•ng **TUYá»†T Äá»I** cá»§a constraint violations trÃªn **Táº¤T Cáº¢** samples trong dataset.

$$
\text{Consistency Error (Total)} = \sum_{i=1}^{N} \left| \sum_{j=1}^{K} \beta_j \cdot p_{j,i} - p_{\text{orig},i} \right|
$$

**KÃ½ hiá»‡u:**
- $N$: Total number of samples (e.g., 68 images)
- $p_{j,i}$: Probability cá»§a component $j$ cho sample $i$
- $p_{\text{orig},i}$: Original probability cho sample $i$
- $\beta_j$: Importance weight cá»§a component $j$ (normalized: $\sum_{j=1}^K \beta_j = 1$)
- $|\cdot|$: Absolute value - chá»‰ Ä‘o **magnitude** violation (khÃ´ng quan tÃ¢m dáº¥u)

**VÃ­ dá»¥:**
- Config D: Consistency Error Total = **1.728** trÃªn 68 samples
- Config E: Consistency Error Total = **1.667** trÃªn 68 samples (tá»‘t hÆ¡n)

** LÆ¯U Ã QUAN TRá»ŒNG:**
- ÄÃ¢y lÃ  **SUM**, khÃ´ng pháº£i **MEAN** â†’ phá»¥ thuá»™c vÃ o sá»‘ samples $N$
- GiÃ¡ trá»‹ lá»›n khÃ´ng nháº¥t thiáº¿t nghÄ©a lÃ  tá»“i náº¿u $N$ lá»›n
- Cáº§n xem **Per-Image Average** Ä‘á»ƒ interpret Ä‘Ãºng

**Má»¥c tiÃªu:** Minimize (ideal: < 5 cho 68 samples â‰ˆ 0.07 per image).

---

#### 6.3.2. Consistency Error Per-Image Average (Mean Constraint Violation)

**Äá»‹nh nghÄ©a:** Trung bÃ¬nh **TUYá»†T Äá»I** constraint violation **má»—i sample**.

**CÃ´ng thá»©c Ä‘áº§y Ä‘á»§:**

$$
\text{Consistency Error (Per-Image Avg)} = \frac{1}{N} \sum_{i=1}^{N} \left| \sum_{j=1}^{K} \beta_j \cdot p_{j,i} - p_{\text{orig},i} \right|
$$

**Relationship vá»›i Total:**

$$
\text{Per-Image Avg} = \frac{\text{Consistency Error Total}}{N}
$$

**VÃ­ dá»¥:**
- Config D: $\frac{1.728}{68} = 0.02542$ â‰ˆ **2.54%** probability deviation
- Config E: $\frac{1.667}{68} = 0.02452$ â‰ˆ **2.45%** probability deviation (tá»‘t hÆ¡n)

**Interpretation:**
- **< 0.05 (5%)**: Excellent - constraint gáº§n nhÆ° hoÃ n háº£o
- **0.05 - 0.10 (5-10%)**: Good - váº«n cháº¥p nháº­n Ä‘Æ°á»£c
- **0.10 - 0.20 (10-20%)**: Marginal - cáº§n cáº£i thiá»‡n
- **> 0.20 (20%)**: Poor - violation quÃ¡ lá»›n, decomposition khÃ´ng reliable

**Ã nghÄ©a thá»±c táº¿:**
- Per-Image Avg = 0.02452 nghÄ©a lÃ : "Trung bÃ¬nh má»—i áº£nh cÃ³ sai lá»‡ch ~2.45% confidence giá»¯a tá»•ng components vÃ  original"
- ÄÃ¢y lÃ  metric **QUAN TRá»ŒNG NHáº¤T** Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ consistency quality

**Má»¥c tiÃªu:** Minimize (ideal: < 0.05 = 5%).

---

#### 6.3.3. Consistency Accuracy (1 - Error Rate)

**Äá»‹nh nghÄ©a:** % samples cÃ³ constraint violation **NHá» HÆ N** threshold $\tau$.

$$
\text{Consistency Accuracy} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{1}\left[\left| \sum_{j=1}^{K} \beta_j \cdot p_{j,i} - p_{\text{orig},i} \right| < \tau \right] \times 100\%
$$

ThÆ°á»ng dÃ¹ng $\tau = 0.05$ (5% tolerance).

**Relationship vá»›i Error Rate:**

$$
\text{Consistency Accuracy} = (1 - \text{Error Rate}) \times 100\%
$$

$$
\text{Error Rate} = \frac{\text{Number of samples with violation} \geq \tau}{N}
$$

**VÃ­ dá»¥:**
- Config D: Accuracy = **97.46%** â†’ Error Rate = 2.54% â†’ 2 samples (trong 68) vi pháº¡m > 5%
- Config E: Accuracy = **97.55%** â†’ Error Rate = 2.45% â†’ 2 samples (trong 68) vi pháº¡m > 5%

**Interpretation:**
- **> 95%**: Excellent - háº§u háº¿t samples thá»a constraint
- **90-95%**: Good - cháº¥p nháº­n Ä‘Æ°á»£c
- **80-90%**: Marginal - cÃ³ ~10 samples problematic
- **< 80%**: Poor - quÃ¡ nhiá»u samples vi pháº¡m

** LÆ¯U Ã:**
- "Error" trong "Accuracy (1 - error)" nghÄ©a lÃ  **Error Rate** (% samples vi pháº¡m)
- KhÃ´ng pháº£i lÃ  "Consistency Error Total" hay "Per-Image Average"
- ÄÃ¢y lÃ  **binary metric**: sample hoáº·c pass ($< \tau$) hoáº·c fail ($\geq \tau$)

**Ã nghÄ©a thá»±c táº¿:**
- Accuracy = 97.55% nghÄ©a lÃ : "66 trong 68 áº£nh (97.55%) cÃ³ violation < 5%, chá»‰ 2 áº£nh vi pháº¡m"
- Nhá»¯ng 2 áº£nh vi pháº¡m cÃ³ thá»ƒ do: cháº¥t lÆ°á»£ng áº£nh tháº¥p, ambiguous objects, hoáº·c convergence issue

**Má»¥c tiÃªu:** Maximize (ideal: > 90%).

---

#### 6.3.4. So SÃ¡nh 3 Metrics Consistency

| Metric | CÃ´ng Thá»©c | ÄÆ¡n vá»‹ | Ã nghÄ©a | Má»¥c tiÃªu |
|--------|-----------|---------|---------|----------|
| **Total Error** | $\sum_i \|v_i\|$ | Absolute sum | Tá»•ng vi pháº¡m toÃ n dataset | Minimize < 5 |
| **Per-Image Avg** | $\frac{1}{N}\sum_i \|v_i\|$ | Probability (0-1) | Trung bÃ¬nh vi pháº¡m má»—i áº£nh | Minimize < 0.05 |
| **Accuracy** | $\frac{1}{N}\sum_i \mathbb{1}[\|v_i\| < \tau]$ | Percentage (0-100%) | % samples pass threshold | Maximize > 95% |

**Relationship:**

```
Total Error = Per-Image Avg Ã— N
Accuracy = 100% - Error Rate
Error Rate = % samples vá»›i |v_i| â‰¥ Ï„
```

**VÃ­ dá»¥ Config E (68 samples):**
- Total = 1.667
- Per-Image Avg = 1.667 / 68 = 0.02452 (2.45%)
- Accuracy = 97.55% (66/68 samples vá»›i violation < 5%)
- Error Rate = 2.45% (2/68 samples vá»›i violation â‰¥ 5%)

**Khi nÃ o dÃ¹ng metric nÃ o:**
- **Total Error:** So sÃ¡nh configs vá»›i **CÃ™NG** dataset size (68 samples)
- **Per-Image Avg:** So sÃ¡nh configs vá»›i **KHÃC** dataset size, hoáº·c interpret violation magnitude
- **Accuracy:** ÄÃ¡nh giÃ¡ robustness - bao nhiÃªu % samples reliable

---

#### 6.3.5. Output Format trong metrics_summary.txt

**VÃ­ dá»¥ output thá»±c táº¿ tá»« Multi-Component OptiCAM:**

```
-- Consistency Constraint (Thesis Objective) --
Consistency error |Î£c_k - c|     : 1.728253
  Per-image average              : 0.025415
  Accuracy (1 - error)           : 97.46%
```

**Giáº£i thÃ­ch tá»«ng dÃ²ng:**

1. **"Consistency error |Î£c_k - c|"** = **Consistency Error Total**
   - KÃ½ hiá»‡u cÅ©: $|Î£c_k - c|$ = $|\sum_{j=1}^K \beta_j \cdot p_j - p_{\text{orig}}|$
   - **NOTE:** $c_k$ trong output nghÄ©a lÃ  $\beta_k \cdot p_k$ (weighted component score)
   - GiÃ¡ trá»‹: **1.728253** (tá»•ng absolute violation trÃªn 68 samples)

2. **"Per-image average"** = **Consistency Error Per-Image Average**
   - CÃ´ng thá»©c: Total / N = 1.728253 / 68 = **0.025415**
   - Interpretation: Trung bÃ¬nh má»—i áº£nh sai lá»‡ch ~2.54% confidence
   - Threshold tá»‘t: < 0.05 (5%)

3. **"Accuracy (1 - error)"** = **Consistency Accuracy**
   - GiÃ¡ trá»‹: **97.46%** (66 trong 68 samples pass threshold Ï„=0.05)
   - Error Rate = 1 - 0.9746 = 0.0254 = 2.54% (2 samples fail)
   - "error" á»Ÿ Ä‘Ã¢y nghÄ©a lÃ  **Error Rate** (% samples vi pháº¡m threshold)

** AMBIGUITY TRONG TÃŠN Gá»ŒI:**

| Term trong output | TÃªn Ä‘áº§y Ä‘á»§ trong lÃ½ thuyáº¿t | ÄÆ¡n vá»‹ | Ã nghÄ©a |
|-------------------|----------------------------|---------|---------|
| "Consistency error" | Consistency Error **Total** | Absolute sum | Tá»•ng vi pháº¡m |
| "Per-image average" | Consistency Error **Per-Image Avg** | Probability | Vi pháº¡m trung bÃ¬nh má»—i áº£nh |
| "Accuracy (1 - error)" | Consistency **Accuracy** | Percentage | % samples pass threshold |

**LÃ½ do gÃ¢y nháº§m láº«n:**
- "Error" xuáº¥t hiá»‡n á»Ÿ 2 contexts khÃ¡c nhau:
  1. **"Consistency error"** = magnitude cá»§a violation (total hoáº·c per-image)
  2. **"error" trong "(1 - error)"** = Error Rate (% samples fail)
- File lÃ½ thuyáº¿t Ä‘Ã£ lÃ m rÃµ báº±ng cÃ¡ch tÃ¡ch thÃ nh 3 metrics riÃªng biá»‡t (6.3.1, 6.3.2, 6.3.3)

**Mapping chuáº©n:**

```python
# Trong code
consistency_error_total = sum(abs(violations))  # 1.728253
per_image_avg = consistency_error_total / N     # 0.025415
accuracy = (samples_pass / N) * 100             # 97.46%
error_rate = 1 - (accuracy / 100)               # 0.0254
```

**VÃ­ dá»¥ Ä‘á»c output:**

> "Config D cÃ³ Consistency error = 1.728, Per-image average = 0.025, Accuracy = 97.46%"

**Interpretation:**
- Tá»•ng vi pháº¡m = 1.728 trÃªn 68 áº£nh
- Má»—i áº£nh vi pháº¡m trung bÃ¬nh 2.54% confidence (ráº¥t tá»‘t, < 5%)
- 66/68 áº£nh (97.46%) cÃ³ vi pháº¡m nhá» hÆ¡n threshold 5%
- Chá»‰ 2 áº£nh vi pháº¡m > 5% (cÃ³ thá»ƒ do áº£nh cháº¥t lÆ°á»£ng tháº¥p)

---

#### 6.3.6. LiÃªn Há»‡ Vá»›i Consistency Loss Trong Training

**Consistency Loss (training objective - Section 4.2.2):**

$$
\mathcal{L}_{\text{consistency}} = \frac{1}{B} \sum_{i=1}^{B} \left( p_{\text{orig},i} - \sum_{j=1}^{K} \beta_j \cdot p_{j,i} \right)^2
$$

**Consistency Error (evaluation metric - Section 6.3.1-6.3.2):**

$$
\text{Consistency Error Total} = \sum_{i=1}^{N} \left| p_{\text{orig},i} - \sum_{j=1}^{K} \beta_j \cdot p_{j,i} \right|
$$

**So sÃ¡nh:**

| Aspect | Consistency Loss (Training) | Consistency Error (Evaluation) |
|--------|----------------------------|--------------------------------|
| **Má»¥c Ä‘Ã­ch** | Optimize weights $\mathbf{U}, \boldsymbol{\beta}$ | Äo violation sau training |
| **Timing** | TÃ­nh **Má»–I iteration** (100 iterations) | TÃ­nh **1 láº§n** sau converge |
| **Function** | Squared error: $(v)^2$ | Absolute error: $|v|$ |
| **Why squared?** | Smooth gradient cho optimization | Interpretable magnitude |
| **Scope** | Per-batch (B images, e.g., 10) | ToÃ n dataset (N images, e.g., 68) |
| **Aggregation** | Mean over batch: $\frac{1}{B}\sum$ | Sum over dataset: $\sum$ |
| **Scale** | Small (~0.001-0.01) do squared + mean | Lá»›n hÆ¡n (~1-2) do absolute + sum |

**VÃ­ dá»¥ cá»¥ thá»ƒ:**

**Iteration 50 (training):**
```python
violations = [0.02, -0.03, 0.01, ..., 0.04]  # Batch size B=10
consistency_loss = mean(violations**2) = 0.0008  # MSE
```

**Sau training (evaluation):**
```python
violations = [0.02, 0.03, 0.01, ..., 0.04]  # All N=68 samples
consistency_error_total = sum(abs(violations)) = 1.728
per_image_avg = 1.728 / 68 = 0.025
```

**LÃ½ do khÃ¡c nhau:**
- **Loss (squared):** Penalize large violations hÆ¡n â†’ gradient lá»›n hÆ¡n â†’ faster correction
- **Error (absolute):** Äo magnitude tháº­t â†’ dá»… interpret (2.5% deviation vs 0.0625% squared deviation)

**Má»¥c tiÃªu chung:** Cáº£ 2 Ä‘á»u muá»‘n **minimize** - violation cÃ ng nhá» cÃ ng tá»‘t!


---

## 7. Váº¥n Äá» Quan Trá»ng: num_masks - K Components vs C Channels

### 7.1. Äá»‹nh NghÄ©a vÃ  PhÃ¢n Biá»‡t

#### 7.1.1. Channels (C) - Feature Map Dimension (Tá»« OptiCAM Paper)

**Äá»‹nh nghÄ©a (theo paper Equation 8):** Sá»‘ channels trong feature map cá»§a target layer $\ell$.

Layer $\ell$ vá»›i $K_\ell$ channels cÃ³ feature maps:

$$
A^k_\ell \in \mathbb{R}^{h_\ell \times w_\ell} \quad \text{for } k = 1, \ldots, K_\ell
$$

**VÃ­ dá»¥:**
- ResNet50 `layer4[-1]`: $K_\ell = 2048$ channels
- VGG16 `features[28]`: $K_\ell = 512$ channels

**Má»—i channel $A^k_\ell$ captures má»™t feature detector:**
- Channel 1: Horizontal edges
- Channel 2: Circular patterns
- Channel 512: High-level object parts
- Channel 2048: Complex semantic features

**Vai trÃ²:** LÃ  **input** cho optimization - raw features tá»« pre-trained network.

**KÃ½ hiá»‡u trong paper:** $K_\ell$ (sá»‘ channels cá»§a layer $\ell$)
**KÃ½ hiá»‡u trong code:** `C` hoáº·c `num_features` (e.g., C=2048 cho ResNet50 layer4)

#### 7.1.2. Components (K) - Learnable Semantic Groups (Multi-Component Extension)

**Äá»‹nh nghÄ©a:** Sá»‘ lÆ°á»£ng **saliency masks riÃªng biá»‡t** Ä‘Æ°á»£c há»c tá»« feature maps.

$$
K = \text{num\_masks} \quad (\text{hyperparameter - do user chá»n})
$$

**Current implementation:** $K = 3$ (3 components).

**Má»—i component lÃ  weighted combination of ALL $K_\ell$ channels (má»Ÿ rá»™ng Equation 8):**

$$
S^{(j)}_\ell(\mathbf{x}; \mathbf{u}_j) = h\left(\sum_{k=1}^{K_\ell} w_{j,k} \cdot A^k_\ell\right) \quad \text{for } j=1,...K
$$

Vá»›i $K_\ell = 2048$ (ResNet50 layer4).

**Vai trÃ²:** LÃ  **output** cá»§a optimization - learned decomposition thÃ nh $K$ semantic parts.

**KÃ½ hiá»‡u:**
- Paper OptiCAM baseline: KhÃ´ng cÃ³ $K$ (chá»‰ 1 mask)
- Multi-Component extension: $K$ (sá»‘ components), subscript $j$ Ä‘á»ƒ index
- TrÃ¡nh nháº§m láº«n: $K$ (components) $\neq$ $K_\ell$ (channels)

### 7.2. ToÃ¡n Há»c: K Components vs C Channels

#### 7.2.1. Current Implementation (K=3 Components)

**Learnable weights:**

$$
W_{\text{raw}} \in \mathbb{R}^{K \times C \times 1 \times 1} = \mathbb{R}^{3 \times 2048 \times 1 \times 1}
$$

**Softmax normalization per component:**

$$
w_{j,k} = \frac{\exp(u_{j,k})}{\sum_{k'=1}^{2048} \exp(u_{j,k'})} \quad \text{for } j=1,2,3
$$

**Component j lÃ  linear combination:**

$$
\text{mask}_j = \sum_{k=1}^{2048} w_{j,k} \cdot \text{channel}_k
$$

**Consistency constraint:**

$$
\sum_{j=1}^{3} \beta_j \cdot p(\text{mask}_j) \approx p(\text{original})
$$

**Computational cost:**
- K+1 forward passes per iteration = 4 forwards (3 components + 1 combined)
- Total per image: $4 \times 100 \text{ iters} = 400$ forwards
- Time: ~14 minutes / 70 images

#### 7.2.2. Thesis Goal ($K_\ell$=2048 "Channels RiÃªng Biá»‡t")

**YÃªu cáº§u giáº£ng viÃªn:** "Äá»‘i vá»›i **tá»«ng channel riÃªng biá»‡t**, khi mask lÃªn áº£nh vÃ  qua classifier, confidence $c_k$ cÃ³ tá»•ng $\sum c_k = c_{\text{original}}$."

**Interpretation:** Má»—i channel $A^k_\ell$ lÃ  má»™t mask riÃªng biá»‡t â†’ $K = K_\ell = 2048$.

**KhÃ´ng cáº§n learn weights - dÃ¹ng trá»±c tiáº¿p feature maps:**

$$
S^{(k)}_\ell(\mathbf{x}) = n(\text{up}(A^k_\ell)) \quad \text{for } k=1..2048
$$

Vá»›i $n(\cdot)$ lÃ  normalization (Equation 4), $\text{up}(\cdot)$ lÃ  upsampling.

**LÆ°u Ã½:** á» Ä‘Ã¢y $k$ vá»«a lÃ  **channel index** vá»«a lÃ  **mask index** vÃ¬ má»—i channel táº¡o ra 1 mask riÃªng.

**Consistency constraint (Ä‘Æ¡n giáº£n hÆ¡n - PURE SUM):**

$$
\sum_{k=1}^{2048} p(S^{(k)}_\ell) \approx p(\mathbf{x})
$$

**LÆ°u Ã½:** KhÃ´ng cÃ³ weights $\beta_j$ - má»—i channel Ä‘Ã³ng gÃ³p báº±ng nhau (hoáº·c cÃ³ thá»ƒ thÃªm learned $\beta$ sau).

**Computational cost:**
- $K_\ell + 1$ forward passes per iteration = 2049 forwards
- Total per image: $2049 \times 100 = 204,900$ forwards
- Time estimate: **~4 days / 70 images** (410x cháº­m hÆ¡n K=3)

### 7.3. So SÃ¡nh Pipeline: K=3 vs $K_\ell$=2048

#### 7.3.1. Pipeline Hiá»‡n Táº¡i (K=3 Components)

```
Input x âˆˆ â„^(3Ã—224Ã—224)
    â†“
ResNet50 layer4[-1]
    â†“
Features {A^k_â„“}_{k=1}^{K_â„“=2048} âˆˆ â„^(2048Ã—14Ã—14)   [2048 CHANNELS tá»« pre-trained]
    â†“
Learnable U âˆˆ â„^(3Ã—2048)  [3 COMPONENTS (j), má»—i cÃ¡i learn weights cho ALL 2048 CHANNELS (k)]
    â†“
w_{j,k} = softmax(u_j)_k âˆˆ â„^(3Ã—2048)  [Equation 8: normalize weights, j=component, k=channel]
    â†“
S^(1) = h(Î£_k w_{1,k} Ã— A^k_â„“)  [component 1 combines all 2048 channels]
S^(2) = h(Î£_k w_{2,k} Ã— A^k_â„“)  [component 2 combines all 2048 channels]
S^(3) = h(Î£_k w_{3,k} Ã— A^k_â„“)  [component 3 combines all 2048 channels]
    â†“
3 component scores: p_1, p_2, p_3 (probability space)
    â†“
Consistency: Î£_j (Î²_j Ã— p_j) â‰ˆ p_orig
```

**Äáº·c Ä‘iá»ƒm:**
- âœ… Feasible: 4 forwards Ã— 100 iters = 400 forwards/image (~14 min)
- âœ… Learned semantic groups: Components tá»± Ä‘á»™ng há»c nhÃ³m features cÃ³ liÃªn quan
- âœ… Theo Ä‘Ãºng OptiCAM paper structure (Equation 8) - chá»‰ má»Ÿ rá»™ng ra K láº§n
- âŒ **KhÃ´ng khá»›p thesis goal:** KhÃ´ng pháº£i "tá»«ng channel riÃªng biá»‡t"

#### 7.3.2. Pipeline Theo Má»¥c TiÃªu Luáº­n VÄƒn ($K_\ell$=2048)

```
Input x âˆˆ â„^(3Ã—224Ã—224)
    â†“
ResNet50 layer4[-1]
    â†“
Features {A^k_â„“}_{k=1}^{2048} âˆˆ â„^(2048Ã—14Ã—14)   [2048 CHANNELS]
    â†“
[NO LEARNING] DÃ¹ng trá»±c tiáº¿p tá»«ng channel nhÆ° masks
    â†“
S^(1) = n(up(A^1_â„“))  [channel 1 as mask]
S^(2) = n(up(A^2_â„“))  [channel 2 as mask]
...
S^(2048) = n(up(A^2048_â„“))  [channel 2048 as mask]
    â†“
2048 confidence scores: p_1, p_2, ..., p_2048
    â†“
Consistency: Î£ p_k â‰ˆ p_orig  [PURE SUM, no weights Î²]
```

**Äáº·c Ä‘iá»ƒm:**
- âœ… **Khá»›p thesis goal:** "Tá»«ng channel riÃªng biá»‡t"
- âœ… Mathematically pure: Decomposition theo individual features
- âœ… Váº«n dá»±a trÃªn OptiCAM framework (dÃ¹ng feature maps A^k_â„“)
- âŒ **KhÃ´ng feasible:** 2049 forwards Ã— 100 iters = 204,900 forwards/image (~4 days)
- âŒ KhÃ´ng cÃ³ learning: Features cá»‘ Ä‘á»‹nh tá»« pre-trained model

### 7.4. Báº£ng So SÃ¡nh Chi Tiáº¿t

| Aspect | **Current (K=3)** | **Thesis Goal (C=2048)** | **Compromise (K=32)** |
|--------|-----------------|------------------------|---------------------|
| **Sá»‘ masks** | 3 components | 2048 channels | 32 representative channels |
| **CÃ¡ch táº¡o mask** | Linear combination cá»§a ALL channels | Má»—i channel riÃªng biá»‡t | Chá»n 32 channels quan trá»ng nháº¥t |
| **Learnable weights** | W âˆˆ â„^(3Ã—2048) | None (hoáº·c Identity) | W âˆˆ â„^(32Ã—2048) |
| **Consistency** | Î£(Î²_j Ã— p_j) â‰ˆ p_orig | Î£(c_k) â‰ˆ c_orig | Î£(Î²_j Ã— p_j) â‰ˆ p_orig |
| **Forwards/iter** | 4 | 2049 | 33 |
| **Total forwards** | 400/image | 204,900/image | 3,300/image |
| **Time estimate** | ~14 min / 70 img | ~4 days / 70 img | ~79 min / 70 img |
| **Semantic meaning** | Learned groups (e.g., "head", "body", "background") | Individual features (e.g., "edge detector #512") | Mix of important features |
| **Khá»›p thesis?** | âŒ KhÃ´ng (approximate) | âœ… ÄÃºng 100% | âš ï¸ Gáº§n hÆ¡n (compromise) |


---

# PHá»¤ Lá»¤C: CÃ¢u Há»i Quan Trá»ng vÃ  Giáº£i ÄÃ¡p Chi Tiáº¿t

## CÃ¢u Há»i 1: OptiCAM Baseline Thiáº¿u TÃ­nh Consistency NhÆ° Tháº¿ NÃ o?

### 1.1. Äá»‹nh NghÄ©a Consistency Trong Context Multi-Component

**Consistency constraint** lÃ  yÃªu cáº§u toÃ¡n há»c:

$$
\sum_{j=1}^{K} \beta_j \cdot p_j \approx p_{\text{orig}}
$$

**Giáº£i thÃ­ch kÃ½ hiá»‡u:**
- $K$: Sá»‘ components K
- $p_j = softmax(f(x_j))_c$: Probability cá»§a component $j$ (masked image $j$)
- $p_{orig} = softmax(f(x))_c$: Probability cá»§a áº£nh gá»‘c
- $\beta_j$: Trá»ng sá»‘ má»©c quan trá»ng há»c Ä‘Æ°á»£c (Learnable importance weight) vá»›i $\beta_j \in [0,1]$, $\sum_{j=1}^{K} \beta_j = 1$

**Ã nghÄ©a:**
- Tá»•ng cÃ³ trá»ng sá»‘ cá»§a K component scores â‰ˆ original score
- CÃ¡c components "decompose" prediction thÃ nh cÃ¡c pháº§n Ä‘á»™c láº­p
- Khi "cá»™ng láº¡i" (vá»›i trá»ng sá»‘ $\beta_j$), pháº£i báº±ng original prediction

### 1.2. Táº¡i Sao OptiCAM Baseline THIáº¾U Consistency?

**OptiCAM Baseline cÃ³ 2 objective functions (Equation 10 vÃ  19 trong paper):**

#### Option 1: Default Objective "Mask" (Equation 10)

$$
\mathbf{u}^* = \arg\max_{\mathbf{u}} F^c_\ell(\mathbf{x}; \mathbf{u})
$$

$$
F^c_\ell(\mathbf{x}; \mathbf{u}) = g_c(f(\mathbf{x} \odot n(\text{up}(S_\ell(\mathbf{x}; \mathbf{u})))))
$$

**Ã nghÄ©a:** Maximize logit cá»§a masked image (preserve confidence).

#### Option 2: Alternative Objective "Diff" (Equation 19)

$$
F^c_\ell(\mathbf{x}; \mathbf{u}) := -\left| g_c(f(\mathbf{x})) - g_c(f(\mathbf{x} \odot n(\text{up}(S_\ell(\mathbf{x}; \mathbf{u}))))) \right|
$$

**Ã nghÄ©a:** Minimize difference giá»¯a original logit vÃ  masked logit (preserve prediction).

---

**PhÃ¢n tÃ­ch: Táº¡i sao Cáº¢ HAI Ä‘á»u thiáº¿u Consistency?**

1. **Chá»‰ tá»‘i Æ°u 1 mask duy nháº¥t:** 
   - Cáº£ "Mask" vÃ  "Diff" Ä‘á»u táº¡o **1 saliency map** $S_\ell$ tá»« 1 bá»™ weights $\mathbf{u}$
   - KhÃ´ng cÃ³ khÃ¡i niá»‡m "multiple components" â†’ khÃ´ng thá»ƒ cÃ³ constraint giá»¯a cÃ¡c components

2. **KhÃ´ng cÃ³ decomposition requirement:**
   - **"Mask"**: Maximize $g_c(f(\mathbf{x}_{\text{masked}}))$ - chá»‰ quan tÃ¢m masked score cao
   - **"Diff"**: Minimize $|g_c(f(\mathbf{x})) - g_c(f(\mathbf{x}_{\text{masked}}))|$ - chá»‰ quan tÃ¢m score gáº§n original
   - **Cáº¢ HAI** KHÃ”NG yÃªu cáº§u: mask pháº£i decompose Ä‘Æ°á»£c thÃ nh cÃ¡c pháº§n Ä‘á»™c láº­p
   - **Cáº¢ HAI** KHÃ”NG cÃ³ constraint vá» tá»•ng cÃ¡c pháº§n

3. **KhÃ´ng cÃ³ $\mathcal{L}_{\text{consistency}}$ term:**
   - Baseline (cáº£ 2 objectives): $\mathcal{L} = \mathcal{L}_{\text{fidelity}}$ (1 mask, 1 objective)
   - Multi-Component: $\mathcal{L} = \mathcal{L}_{\text{fidelity}} + \lambda \mathcal{L}_{\text{consistency}}$ (K masks, 2 objectives)

### 1.3. VÃ­ Dá»¥ Minh Há»a: Táº¡i Sao Cáº§n Consistency?

**Scenario: áº¢nh chÃ³ vá»›i K=3 components**

#### KhÃ´ng CÃ³ Consistency Constraint (Baseline Approach):

Náº¿u chá»‰ optimize K=3 masks Ä‘á»™c láº­p vá»›i objective "maximize score":

```
Component 1: highlight toÃ n bá»™ chÃ³ â†’ score = 0.85
Component 2: highlight toÃ n bá»™ chÃ³ â†’ score = 0.85  
Component 3: highlight toÃ n bá»™ chÃ³ â†’ score = 0.85

Problem: 3 masks giá»‘ng nhau, khÃ´ng decompose Ä‘Æ°á»£c!
Tá»•ng: Î²â‚(0.85) + Î²â‚‚(0.85) + Î²â‚ƒ(0.85) = 0.85 (náº¿u Î² uniform)
      NhÆ°ng khÃ´ng cÃ³ constraint nÃ o enforce Ä‘iá»u nÃ y!
```

**Váº¥n Ä‘á»:**
- Optimizer tá»± do chá»n báº¥t ká»³ combination nÃ o maximize individual scores
- KhÃ´ng cÃ³ incentive Ä‘á»ƒ táº¡o **diverse** components
- KhÃ´ng Ä‘áº£m báº£o tÃ­nh "additivity" (cá»™ng láº¡i = original)

#### CÃ³ Consistency Constraint (Multi-Component):

$$
\mathcal{L}_{\text{consistency}} = \mathbb{E}\left[\left(\sum_{j=1}^{K} \beta_j \cdot p_j - p_{\text{orig}}\right)^2\right]
$$

```
Iteration 0 (random init):
  Component 1: random mask â†’ score = 0.3
  Component 2: random mask â†’ score = 0.2
  Component 3: random mask â†’ score = 0.4
  Sum: 0.3 + 0.2 + 0.4 = 0.9
  Original: 0.85
  Violation: |0.9 - 0.85| = 0.05
  L_consistency = 0.05Â² = 0.0025 â†’ gradient signal!

Iteration 50 (learning):
  Component 1: Ä‘áº§u chÃ³ â†’ score = 0.4
  Component 2: thÃ¢n chÃ³ â†’ score = 0.3
  Component 3: background â†’ score = 0.15
  Sum: 0.4 + 0.3 + 0.15 = 0.85 â‰ˆ Original!
  Violation: |0.85 - 0.85| = 0.0
  L_consistency = 0.0 â†’ constraint satisfied!
```

**Lá»£i Ã­ch:**
- âœ… Optimizer bá»‹ **force** pháº£i táº¡o components sao cho tá»•ng = original
- âœ… Components tá»± Ä‘á»™ng há»c Ä‘Æ°á»£c **diverse semantic parts** (vÃ¬ duplicate khÃ´ng hiá»‡u quáº£)
- âœ… Äáº£m báº£o tÃ­nh toÃ¡n há»c: decomposition valid

### 1.4. Code Evidence: Baseline vs Multi-Component

#### OptiCAM Baseline (util.py line 207-310):

```python
def forward(self, images, labels):
    # ... extract features ...
    w = torch.full((B, C, 1, 1), 0.5, ...)  # 1 bá»™ weights cho Táº¤T Cáº¢ channels
    optimizer = optim.Adam([w], lr=self.learning_rate)
    
    for step in range(self.max_iter):
        norm_saliency_map, new_images = self.combine_activations(feature, w, images)
        
        # get_loss() há»— trá»£ 2 objectives:
        # - mode="mask": maximize masked score (Equation 10)
        # - mode="diff": minimize |original - masked| (Equation 19)
        loss = self.get_loss(new_images, predict_labels, f_images)
        # ^^^^^^ CHá»ˆ CÃ“ 1 LOSS: fidelity (1 mask, 1 objective)
        # KHÃ”NG CÃ“ consistency term!
        
        optimizer.zero_grad()
        loss.backward(retain_graph=True)
        optimizer.step()
    
    return norm_saliency_map, new_images  # 1 mask duy nháº¥t
```

**Äiá»ƒm quan trá»ng:**
- `w.shape = (B, C, 1, 1)` - 1 bá»™ weights cho má»—i image
- `loss = get_loss(...)` - chá»‰ cÃ³ fidelity loss (dÃ¹ "mask" hay "diff" objective)
- **KHÃ”NG** cÃ³ term nÃ o liÃªn quan Ä‘áº¿n "tá»•ng cÃ¡c components"
- **Cáº£ 2 objectives Ä‘á»u thiáº¿u consistency** vÃ¬ chá»‰ optimize 1 mask duy nháº¥t

#### Multi-Component OptiCAM (util.py line 680-710):

```python
def forward(self, images, labels):
    # ... extract features ...
    
    # ========== ADAPTIVE INITIALIZATION STRATEGY ==========
    # Goal: Baseline-compatible when K=1, symmetry-breaking when K>1
    # Reference: Glorot & Bengio (2010) - symmetry breaking in neural networks
    
    if self.init_method == 'adaptive':
        if self.k == 1:
            # K=1 MODE: Pure constant (baseline-compatible)
            W_raw = torch.full((B, K, C, 1, 1), 0.5, ...)
        else:
            # K>1 MODE: Constant + tiny noise for symmetry breaking
            W_raw = torch.full((B, K, C, 1, 1), 0.5, ...)
            noise = torch.randn_like(W_raw) * 1e-4  # Tiny Gaussian noise
            W_raw = W_raw + noise
    
    elif self.init_method == 'random':
        # Random Gaussian initialization (original approach)
        W_raw = torch.randn(B, K, C, 1, 1, ...) * 0.01
    
    elif self.init_method == 'constant':
        # Pure constant (âš ï¸ WARNING: symmetry problem if K>1!)
        W_raw = torch.full((B, K, C, 1, 1), 0.5, ...)
        if K > 1:
            print("[WARNING] init_method='constant' with K>1 may cause symmetry!")
    
    W_raw.requires_grad = True  # Set grad after init to ensure leaf tensor
    beta_raw = torch.full((B, K), 1.0/K, ...)  # Initialize beta uniformly
    optimizer = optim.Adam([W_raw, beta_raw], lr=self.learning_rate)
    
    for step in range(self.max_iter):
        masks = self._build_masks_from_channel_weights(feature, images, W_raw)
        # ^^^^^^ Táº¡o K masks riÃªng biá»‡t
        
        # Forward pass cho K components
        x_all = [mask_j * images for mask_j in masks]  # K masked images
        p_j = [model(x_j) for x_j in x_all]            # K scores
        
        # Fidelity loss (combined mask)
        loss_fidelity = (p_combined - p_orig)Â²
        
        # Consistency loss (SUM CONSTRAINT - Má»šI!)
        sum_component_probs = Î£(Î²_j Ã— p_j)
        constraint_violation = sum_component_probs - p_orig
        loss_consistency = constraint_violationÂ²
        # ^^^^^^ ENFORCE: Î£(Î²_j Ã— p_j) â‰ˆ p_orig
        
        # Total loss
        loss = loss_fidelity + Î»_t Ã— loss_consistency
        # ^^^^^^ 2 objectives: faithfulness + decomposition
        
        optimizer.zero_grad()
        loss.backward(retain_graph=True)
        optimizer.step()
    
    return masks  # K masks khÃ¡c biá»‡t
```

### 1.5. TÃ³m Táº¯t: Consistency Constraint

| Aspect | OptiCAM Baseline | Multi-Component OptiCAM |
|--------|------------------|------------------------|
| **Number of masks** | 1 mask | K masks |
| **Objective options** | "Mask" (Eq 10) hoáº·c "Diff" (Eq 19) | Fidelity + Consistency |
| **"Mask" objective** | Maximize $g_c(f(\mathbf{x}_{\text{masked}}))$ | N/A |
| **"Diff" objective** | Minimize $\|g_c(f(\mathbf{x})) - g_c(f(\mathbf{x}_{\text{masked}}))\|$ | Inspiration cho consistency |
| **Consistency term** | âŒ KHÃ”NG CÃ“ (cáº£ 2 objectives) | âœ… $\mathcal{L}_{\text{consistency}} = (\sum \beta_j p_j - p_{\text{orig}})^2$ |
| **Decomposition** | KhÃ´ng yÃªu cáº§u | **ENFORCE** via constraint |
| **Mathematical guarantee** | 1 mask faithful | K masks decompose correctly |
| **Code evidence** | `loss = get_loss(...)` (line 256) | `loss = fidelity + Î»*consistency` (line 708) |

**LÆ°u Ã½ quan trá»ng:** 
- "Diff" objective (Eq 19) cÃ³ Ã½ tÆ°á»Ÿng tÆ°Æ¡ng tá»± consistency (minimize difference)
- **NHÆ¯NG** "Diff" chá»‰ Ã¡p dá»¥ng cho 1 mask duy nháº¥t: $|\text{original} - \text{masked}|$
- Consistency trong Multi-Component má»Ÿ rá»™ng thÃ nh: $|\text{original} - \sum_{j=1}^{K} \beta_j \cdot \text{component}_j|$
- Multi-Component = **generalization** cá»§a "Diff" objective sang K components!
| **Objective** | Maximize $F^c_\ell$ (fidelity only) | Fidelity + Consistency |
| **Consistency term** | âŒ KHÃ”NG CÃ“ | âœ… $\mathcal{L}_{\text{consistency}} = (\sum_{j=1}^{K} \beta_j p_j - p_{\text{orig}})^2$ |
| **Decomposition** | KhÃ´ng yÃªu cáº§u | **ENFORCE** via constraint |
| **Mathematical guarantee** | 1 mask faithful | K masks decompose correctly |
| **Code evidence** | `loss = get_loss(...)` (line 256) | `loss = fidelity + Î»*consistency` (line 708) |

---

## CÃ¢u Há»i 2: Táº¡i Sao Multi Cháº­m HÆ¡n Baseline Náº¿u K Components Giáº£m Computation?

### 2.1. Hiá»ƒu ÄÃºng Vá» "Giáº£m Computation" 

**CÃ¢u claim trong thesis:**

> "Äá» xuáº¥t K components Ä‘á»ƒ giáº£m khá»‘i lÆ°á»£ng tÃ­nh toÃ¡n"

**Ã nghÄ©a ÄÃšNG cá»§a statement nÃ y:**

**KHÃ”NG pháº£i:** "Multi-Component nhanh hÆ¡n OptiCAM Baseline"

**MÃ€ LÃ€:** "K=3 components NHANH HÆ N NHIá»€U so vá»›i K_â„“=2048 channels riÃªng láº»"

### 2.2. So SÃ¡nh 3 Approaches

#### Approach 1: OptiCAM Baseline (1 Mask)

**Pipeline:**
```
Input â†’ Features (C=2048 channels) 
      â†’ Learn 1 bá»™ weights w âˆˆ â„^(2048)
      â†’ Create 1 mask = Î£(w_c Ã— channel_c)
      â†’ Forward pass: 1 masked image
      â†’ Optimize 100 iterations
```

**Computational cost:**
- **Learnable params:** 2,048 weights
- **Forward passes per iteration:** 1 (masked image)
- **Total forwards/image:** ~100 (1 per iteration)
- **Gradient computation:** Backprop through 1 mask
- **Time:** ~**5-7 phÃºt / 70 áº£nh** (baseline reference)

#### Approach 2: Per-Channel Masks (K_â„“=2048 Masks) - THESIS IDEAL?

**Pipeline:**
```
Input â†’ Features (C=2048 channels)
      â†’ Create 2048 masks (1 per channel, no learning)
      â†’ Forward pass: 2048 masked images
      â†’ No optimization (direct from features)
```

**Computational cost:**
- **Learnable params:** 0 (trá»±c tiáº¿p tá»« channels)
- **Forward passes per image:** 2,048 (má»—i channel 1 mask)
- **Total forwards/image:** 2,048 (no iterations needed)
- **Optimization:** KHÃ”NG Cáº¦N (khÃ´ng há»c weights)
- **Time:** ~**4 NGÃ€Y / 70 áº£nh** (204,900 forwards)

**Váº¥n Ä‘á»:** QuÃ¡ cháº­m â†’ khÃ´ng feasible!

#### Approach 3: Multi-Component (K=3 Learned Masks)

**Pipeline:**
```
Input â†’ Features (C=2048 channels)
      â†’ Learn K=3 bá»™ weights U âˆˆ â„^(3Ã—2048)
      â†’ Create K=3 masks = {Î£(w_{k,c} Ã— channel_c)}_{k=1..3}
      â†’ Forward pass: 3 component masks + 1 combined = 4 masked images
      â†’ Optimize 100 iterations
```

**Computational cost:**
- **Learnable params:** 3 Ã— 2,048 = 6,144 weights (+ 3 beta)
- **Forward passes per iteration:** 4 (3 components + 1 combined)
- **Total forwards/image:** ~400 (4 Ã— 100 iterations)
- **Gradient computation:** Backprop through 3 masks + consistency constraint
- **Time:** ~**14 phÃºt / 70 áº£nh** (measured)

### 2.3. Computational Cost Comparison

| Approach | Forwards/Image | Time/70 Images | Speedup vs Per-Channel | Note |
|----------|----------------|----------------|------------------------|------|
| **Baseline (1 mask)** | ~100 | ~5-7 phÃºt | 2,048Ã— faster | âš¡ Nhanh nháº¥t |
| **Multi-Component (K=3)** | ~400 | ~14 phÃºt | **512Ã— faster** | âœ… Giáº£m computation vs 2048 |
| **Per-Channel (K=2048)** | 2,048 | ~4 ngÃ y | 1Ã— (baseline) | âŒ KhÃ´ng feasible |

**TÃ­nh toÃ¡n chi tiáº¿t:**

$$
\text{Speedup} = \frac{2048 \text{ forwards}}{4 \text{ forwards}} = 512\times
$$

### 2.4. Táº¡i Sao Multi Cháº­m HÆ¡n Baseline?

**NguyÃªn nhÃ¢n chÃ­nh:**

#### 1. Nhiá»u Forward Passes HÆ¡n (4Ã— per iteration)

**Baseline:**
```python
for step in range(100):
    mask = create_1_mask(w)
    x_masked = mask * images          # 1 masked image
    score = model(x_masked)            # 1 forward pass
    loss = (score - orig_score)Â²
```

**Multi-Component:**
```python
for step in range(100):
    masks = create_K_masks(W)                    # K=3 masks
    x_components = [mask_j * images for j in K]  # 3 masked images
    x_combined = combined_mask * images          # 1 combined image
    
    # Forward passes
    scores_comp = [model(x_j) for x_j in x_components]  # 3 forwards
    score_comb = model(x_combined)                       # 1 forward
    # TOTAL: 4 forward passes vs baseline's 1
    
    loss_fid = (score_comb - orig)Â²
    loss_cons = (sum(Î²_j * scores_comp[j]) - orig)Â²
    loss = loss_fid + Î» * loss_cons
```

**Forward pass ratio:** 4 : 1 â†’ Multi cáº§n gáº¥p 4Ã— forward passes má»—i iteration

#### 2. Phá»©c Táº¡p HÆ¡n Trong Gradient Computation

**Baseline gradient flow:**
```
loss â†’ score â†’ x_masked â†’ mask â†’ w (2048 params)
     [1 path]
```

**Multi-Component gradient flow:**
```
loss_fid â†’ score_comb â†’ x_comb â†’ mask_comb â†’ {mask_1, ..., mask_K} â†’ W (6144 params)
loss_cons â†’ scores_comp[1..K] â†’ {x_1, ..., x_K} â†’ {mask_1, ..., mask_K} â†’ W
          [K+1 paths, more complex]
```

**Gradient computation overhead:**
- Multi pháº£i backprop qua K+1 forward passes (4 vá»›i K=3)
- Baseline chá»‰ backprop qua 1 forward pass
- Consistency constraint thÃªm computation cho constraint violation term

#### 3. Nhiá»u Learnable Parameters HÆ¡n (3Ã— weights)

| Model | Weights | Beta | Total Params |
|-------|---------|------|--------------|
| Baseline | 2,048 (w) | 0 | 2,048 |
| Multi-Component | 6,144 (W = 3Ã—2048) | 3 (Î²) | 6,147 |

**Optimizer overhead:**
- Adam optimizer pháº£i track momentum vÃ  variance cho má»—i param
- Multi cÃ³ 3Ã— params â†’ 3Ã— memory vÃ  computation trong optimizer step

#### 4. Consistency Constraint Overhead

```python
# Baseline: chá»‰ cÃ³ fidelity loss
loss = (score_masked - score_orig)Â²

# Multi: fidelity + consistency (thÃªm computation)
loss_fidelity = (score_combined - score_orig)Â²
sum_component_probs = sum(beta[j] * scores[j] for j in range(K))  # extra sum
constraint_violation = sum_component_probs - score_orig     # extra subtraction
loss_consistency = constraint_violationÂ²                     # extra square
loss = loss_fidelity + lambda_t * loss_consistency          # extra multiply + add
```

### 2.5. Giáº£i ThÃ­ch "Giáº£m Computation" Statement

**Statement trong thesis ÄÃšNG khi so sÃ¡nh vá»›i approach "per-channel":**

> "Äá»ƒ giáº£m khá»‘i lÆ°á»£ng tÃ­nh toÃ¡n, Ä‘á» xuáº¥t K=3 components thay vÃ¬ sá»­ dá»¥ng trá»±c tiáº¿p K_â„“=2048 channels"

**Table minh há»a:**

| Comparison | Approach A | Approach B | Speedup | Interpretation |
|------------|------------|------------|---------|----------------|
| âœ… **ÄÃšNG** | Multi (K=3, 400 fwd) | Per-Channel (2048 fwd) | **512Ã—** | Giáº£m computation drastically |
| âŒ **SAI** | Multi (K=3, 400 fwd) | Baseline (1, 100 fwd) | 0.25Ã— (slower!) | KhÃ´ng pháº£i comparison nÃ y |

**LÆ°u Ã½:**
- "Giáº£m computation" lÃ  so vá»›i **hypothetical K_â„“=2048 approach**
- KHÃ”NG pháº£i so vá»›i OptiCAM baseline (1 mask)
- Multi trade-off: Cháº­m hÆ¡n baseline nhÆ°ng Ä‘Æ°á»£c **semantic decomposition** + **consistency guarantee**

### 2.6. Trade-off Analysis

#### Option 1: OptiCAM Baseline (Current)
- âš¡ **Nhanh nháº¥t** (~5-7 phÃºt)
- âœ… Saliency map cháº¥t lÆ°á»£ng cao
- âŒ KhÃ´ng decompose Ä‘Æ°á»£c (1 mask)
- âŒ KhÃ´ng phÃ¢n tÃ­ch semantic components

#### Option 2: Multi-Component K=3 (Current)
- ğŸ¢ **Cháº­m hÆ¡n baseline** 2-3Ã— (~14 phÃºt)
- âœ… Decompose thÃ nh 3 semantic parts
- âœ… Consistency constraint (toÃ¡n há»c Ä‘Ãºng)
- âœ… Váº«n feasible (14 phÃºt acceptable)
- âš¡ **Nhanh hÆ¡n per-channel** 512Ã— (4 ngÃ y â†’ 14 phÃºt)

#### Option 3: Per-Channel K=2048 (Hypothetical)
- ğŸŒ **Cá»±c ká»³ cháº­m** (~4 ngÃ y)
- âœ… Äá»™ phÃ¢n giáº£i cao (2048 masks)
- âŒ KhÃ´ng feasible cho research project
- âŒ 2048 masks quÃ¡ nhiá»u Ä‘á»ƒ visualize/interpret

### 2.7. Káº¿t Luáº­n: Giáº£i ThÃ­ch Cho Advisor

**CÃ¢u tráº£ lá»i Ä‘áº§y Ä‘á»§:**

> "ThÆ°a tháº§y, statement 'K components giáº£m computation' lÃ  **so sÃ¡nh vá»›i approach sá»­ dá»¥ng trá»±c tiáº¿p 2048 channels** (má»—i channel 1 mask riÃªng), khÃ´ng pháº£i so vá»›i OptiCAM baseline.
>
> **Chi tiáº¿t:**
> - OptiCAM baseline: 1 mask, ~100 forwards, **5-7 phÃºt** (nhanh nháº¥t)
> - Multi-Component K=3: 3 masks, ~400 forwards, **14 phÃºt** (cháº­m hÆ¡n baseline 2-3Ã—)
> - Per-channel 2048: 2048 masks, ~2048 forwards, **4 ngÃ y** (khÃ´ng feasible)
>
> **Trade-off:**
> - Multi **cháº­m hÆ¡n baseline** vÃ¬: 4Ã— forward passes/iteration, consistency constraint overhead, 3Ã— parameters
> - Multi **nhanh hÆ¡n per-channel** 512Ã— (giáº£m tá»« 4 ngÃ y xuá»‘ng 14 phÃºt)
> - Äá»•i láº¡i: Multi cÃ³ **semantic decomposition** vÃ  **consistency guarantee** (baseline khÃ´ng cÃ³)
>
> **LÃ½ do chá»n K=3:**
> - Váº«n feasible (14 phÃºt acceptable cho research)
> - Äáº¡t Ä‘Æ°á»£c má»¥c tiÃªu decomposition (3 semantic parts)
> - Trade-off há»£p lÃ½: Hy sinh 2-3Ã— runtime Ä‘á»ƒ cÃ³ thÃªm tÃ­nh nÄƒng decomposition"

---

**Káº¿t thÃºc tÃ i liá»‡u.**
